{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70df0e83-9a35-41b3-81ec-cd12538045ed",
   "metadata": {},
   "source": [
    "## VPRTempoQuant - Training and Inferencing Tutorial\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Traditional methods for visual place recognition (VPR) tasks typically employ the use of convolutional neural networks like ResNet to train large datasets for feature extraction of incoming query images, rather than specifically learning said query place. The networks are extremely effective at accurate localisation, but are are slow to train, inference, and store.\n",
    "\n",
    "Spiking neural networks (SNNs) by contrast are more energy efficient and have low latency computation, meaning their deployment capability for VPR is extremely promising. Specifically, networks can be trained on the exact location you wish to query which takes a fundamentally different approach to the VPR task.\n",
    "\n",
    "VPRTempo uses a temporal encoding scheme for spikes, where the amplitude of a spike is determined by an incoming training or query image's pixel intensity. This amplitude defines the 'timing' of the spike, similar to a latency code. As spikes propagate throughout the system, spike-timing dependent plasticity (STDP) learning rules train neuronal connections based off of the pixel intensity spike amplitudes. \n",
    "\n",
    "In this tutorial, we are going to take the base VPRTempo model to train and inference a network with PyTorch's Quantized Aware Training ([QAT](https://pytorch.org/docs/stable/quantization.html)). \n",
    "\n",
    "To get started, please ensure you have installed and currently have activated the `conda` environment for VPRTempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f846c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate vprtempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928d7a4",
   "metadata": {},
   "source": [
    "## 1. Get the Nordland dataset\n",
    "\n",
    "### 1.1 Download the dataset\n",
    "\n",
    "Please [download the Nordland datasets](https://webdiis.unizar.es/~jmfacil/pr-nordland/#download-dataset) (Summer, Spring, Fall, & Winter). There are two datasets available, the full size and downsampled versions. Either will work fine but our paper details the full size dataset. If disk space is a concern, please use the downsampled version.\n",
    "\n",
    "Save the data in the `./VPRTempo-quant/dataset/` subfolder.\n",
    "\n",
    "### 1.2 Prepare the dataset for the model\n",
    "\n",
    "The datset seasons are downloaded in .zip format and need to be extracted into a single folder. The `nordland` function has been provided to automatically do this for you and to re-name the images to match those in the nordland.csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51f350d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Please set the outDir to the desired output location for unzipping the Nordland datasets",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnordland\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nord_sort\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# unzip, re-organise, and re-name the Nordland datasets\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m nord_sort()\n",
      "File \u001b[0;32m~/repos/VPRTempo-quant/./src/nordland.py:33\u001b[0m, in \u001b[0;36mnord_sort\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# set the desired output folder for unzipping and organization\u001b[39;00m\n\u001b[1;32m     32\u001b[0m outDir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misdir(outDir)),\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease set the outDir to the desired output location for unzipping the Nordland datasets\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# define output paths for the data\u001b[39;00m\n\u001b[1;32m     36\u001b[0m outPath \u001b[38;5;241m=\u001b[39m [os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspring/\u001b[39m\u001b[38;5;124m\"\u001b[39m),os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfall/\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     37\u001b[0m            os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwinter/\u001b[39m\u001b[38;5;124m\"\u001b[39m),os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(outDir,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummer/\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n",
      "\u001b[0;31mAssertionError\u001b[0m: Please set the outDir to the desired output location for unzipping the Nordland datasets"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import zipfile\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "sys.path.append('./VPRTempo-quant/dataset')\n",
    "\n",
    "from os import walk\n",
    "from nordland import nord_sort\n",
    "\n",
    "# unzip, re-organise, and re-name the Nordland datasets\n",
    "nord_sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a607d1",
   "metadata": {},
   "source": [
    "## Prepare the model for training\n",
    "\n",
    "Let's now look at preparing our network to train our first model. There are a few initial steps to take care of first.\n",
    "\n",
    "### 2.1 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9caff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import jdc\n",
    "import torch.nn as nn\n",
    "import blitnet as bn\n",
    "import numpy as np\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "from tqdm import tqdm\n",
    "from settings import configure, image_csv, model_logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2f885",
   "metadata": {},
   "source": [
    "### 2.2 Define and initialize the VPRTempo model\n",
    "\n",
    "We'll first define the VPRTempo class which handles the configuration as set in `./src/settings.py`, determining which images to load, and establishes the layers used for training. For this tutorial, leave the settings as the default.\n",
    "\n",
    "`__init__` is where we define the layers used for the model. In this case, we define a `feature_layer` and an `output_layer`. `dims` represents the number of neurons in the input and the layer itself, which in this case is `self.input`, `self.feature`, and `self.output`. Note that the size of the input for each proceeding layer is the size of previous layer. In this example, we have an input of 784 neurons (for 28x28 images) connected to a 1568 neuron feature layer which then connects to a final output layer of 500 neurons.\n",
    "\n",
    "The other hyperparameters for each layer are set here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b5130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPRTempo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VPRTempo, self).__init__()\n",
    "\n",
    "        # Configure the network\n",
    "        configure(self)\n",
    "        \n",
    "        # Define the images to load (both training and inference)\n",
    "        image_csv(self)\n",
    "\n",
    "        # Add quantization stubs for Quantization Aware Training (QAT)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        \n",
    "        # Define the add function for quantized addition\n",
    "        self.add = nn.quantized.FloatFunctional()      \n",
    "\n",
    "        # Layer dict to keep track of layer names and their order\n",
    "        self.layer_dict = {}\n",
    "        self.layer_counter = 0\n",
    "\n",
    "        \"\"\"\n",
    "        Define trainable layers here\n",
    "        \"\"\"\n",
    "        self.add_layer(\n",
    "            'feature_layer',\n",
    "            dims=[self.input, self.feature],\n",
    "            thr_range=[0, 0.5],\n",
    "            fire_rate=[0.2, 0.9],\n",
    "            ip_rate=0.15,\n",
    "            stdp_rate=0.005,\n",
    "            const_inp=[0, 0.1],\n",
    "            p=[0.1, 0.5]\n",
    "        )\n",
    "        self.add_layer(\n",
    "            'output_layer',\n",
    "            dims=[self.feature, self.output],\n",
    "            ip_rate=0.15,\n",
    "            stdp_rate=0.005,\n",
    "            spk_force=True\n",
    "        )\n",
    "    def add_layer(self, name, **kwargs):\n",
    "        \"\"\"\n",
    "        Dynamically add a layer with given name and keyword arguments.\n",
    "\n",
    "        :param name: Name of the layer to be added\n",
    "        :type name: str\n",
    "        :param kwargs: Hyperparameters for the layer\n",
    "        \"\"\"\n",
    "        # Check for layer name duplicates\n",
    "        if name in self.layer_dict:\n",
    "            raise ValueError(f\"Layer with name {name} already exists.\")\n",
    "\n",
    "        # Add a new SNNLayer with provided kwargs\n",
    "        setattr(self, name, bn.SNNLayer(**kwargs))\n",
    "\n",
    "        # Add layer name and index to the layer_dict\n",
    "        self.layer_dict[name] = self.layer_counter\n",
    "        self.layer_counter += 1  \n",
    "\n",
    "        print('Succesfully added '+name)\n",
    "\n",
    "    def train_model(self, train_loader, layer, prev_layers=None):\n",
    "        \"\"\"\n",
    "        Train a layer of the network model.\n",
    "\n",
    "        :param train_loader: Training data loader\n",
    "        :param layer: Layer to train\n",
    "        :param prev_layers: Previous layers to pass data through\n",
    "        \"\"\"\n",
    "\n",
    "        # Initialize the tqdm progress bar\n",
    "        pbar = tqdm(total=int(self.T * self.epoch),\n",
    "                    desc=\"Training \",\n",
    "                    position=0)\n",
    "\n",
    "        # Initialize the learning rates for each layer (used for annealment)\n",
    "        init_itp = layer.eta_ip.detach()\n",
    "        init_stdp = layer.eta_stdp.detach()\n",
    "\n",
    "        # Run training for the specified number of epochs\n",
    "        for epoch in range(self.epoch):\n",
    "            mod = 0  # Used to determine the learning rate annealment, resets at each epoch\n",
    "            # Run training for the specified number of timesteps\n",
    "            for spikes, labels in train_loader:\n",
    "                spikes, labels = spikes.to(self.device), labels.to(self.device)\n",
    "                idx = labels / self.filter # Set output index for spike forcing\n",
    "                # Pass through previous layers if they exist\n",
    "                if prev_layers:\n",
    "                    with torch.no_grad():\n",
    "                        for prev_layer_name in prev_layers:\n",
    "                            prev_layer = getattr(self, prev_layer_name) # Get the previous layer object\n",
    "                            spikes = self.forward(spikes, prev_layer) # Pass spikes through the previous layer\n",
    "                            spikes = bn.clamp_spikes(spikes, prev_layer) # Clamp spikes [0, 0.9]\n",
    "                else:\n",
    "                    prev_layer = None\n",
    "                # Get the output spikes from the current layer\n",
    "                pre_spike = spikes.detach() # Previous layer spikes for STDP\n",
    "                spikes = self.forward(spikes, layer) # Current layer spikes\n",
    "                spikes_noclp = spikes.detach() # Used for inhibitory homeostasis\n",
    "                spikes = bn.clamp_spikes(spikes, layer) # Clamp spikes [0, 0.9]\n",
    "                # Calculate STDP\n",
    "                layer = bn.calc_stdp(pre_spike,spikes,spikes_noclp,layer, idx, prev_layer=prev_layer)\n",
    "                # Adjust learning rates\n",
    "                layer = self._anneal_learning_rate(layer, mod, init_itp, init_stdp)\n",
    "                # Update the annealing mod & progress bar \n",
    "                mod += 1\n",
    "                pbar.update(1)\n",
    "\n",
    "        # Close the tqdm progress bar\n",
    "        pbar.close()\n",
    "\n",
    "    def forward(self, spikes, layer):\n",
    "        \"\"\"\n",
    "        Compute the forward pass of the model.\n",
    "\n",
    "        Parameters:\n",
    "        - spikes (Tensor): Input spikes.\n",
    "\n",
    "        Returns:\n",
    "        - Tensor: Output after processing.\n",
    "        \"\"\"\n",
    "\n",
    "        spikes = self.quant(spikes)\n",
    "        spikes = self.add.add(layer.exc(spikes), layer.inh(spikes))\n",
    "        spikes = self.dequant(spikes)\n",
    "\n",
    "        return spikes\n",
    "\n",
    "    def _anneal_learning_rate(self, layer, mod, itp, stdp):\n",
    "        \"\"\"\n",
    "        Anneal the learning rate for the current layer.\n",
    "        \"\"\"\n",
    "        if np.mod(mod, 100) == 0: # Modify learning rate every 100 timesteps\n",
    "            pt = pow(float(self.T - mod) / self.T, self.annl_pow)\n",
    "            layer.eta_ip = torch.mul(itp, pt) # Anneal intrinsic threshold plasticity learning rate\n",
    "            layer.eta_stdp = torch.mul(stdp, pt) # Anneal STDP learning rate\n",
    "\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bbfd06",
   "metadata": {},
   "source": [
    "Layers are dynamically added, such that if you wish to add more layers you simply need to define one in the `__init__` script and the system will iterate through all the layers.\n",
    "\n",
    "To add the layers, we call the `add_layer()` function which will set all the hyperparameters and seed the initial weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbdd4ffd",
   "metadata": {},
   "source": [
    "Now, we can initialize the model and add the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "68a22d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def forward(self, spikes, layer):\n",
    "    \"\"\"\n",
    "    Compute the forward pass of the model.\n",
    "\n",
    "    Parameters:\n",
    "    - spikes (Tensor): Input spikes.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: Output after processing.\n",
    "    \"\"\"\n",
    "\n",
    "    spikes = self.quant(spikes)\n",
    "    spikes = self.add.add(layer.exc(spikes), layer.inh(spikes))\n",
    "    spikes = self.dequant(spikes)\n",
    "\n",
    "    return spikes\n",
    "\n",
    "def _anneal_learning_rate(self, layer, mod, itp, stdp):\n",
    "    \"\"\"\n",
    "    Anneal the learning rate for the current layer.\n",
    "    \"\"\"\n",
    "    if np.mod(mod, 100) == 0: # Modify learning rate every 100 timesteps\n",
    "        pt = pow(float(self.T - mod) / self.T, self.annl_pow)\n",
    "        layer.eta_ip = torch.mul(itp, pt) # Anneal intrinsic threshold plasticity learning rate\n",
    "        layer.eta_stdp = torch.mul(stdp, pt) # Anneal STDP learning rate\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55aa0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully added feature_layer\n",
      "Succesfully added output_layer\n"
     ]
    }
   ],
   "source": [
    "model = VPRTempo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17640d20",
   "metadata": {},
   "source": [
    "### 2.3 Set the DataLoader\n",
    "\n",
    "Now that we've defined the model, we will set up the DataLoaders. These utilise a PyTorch CustomImageDataset and ProcessImage to import images and process them for training or inference. In brief, images are loaded, gamma corrected, resized, and then patch-normalized before being converted into system spikes to be propagated throughout.\n",
    "\n",
    "Since we present the network with one image at a time, the `batch_size` is kept to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6714bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "train_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                       img_dirs=model.training_dirs,\n",
    "                                       transform=image_transform,\n",
    "                                       skip=model.filter,\n",
    "                                       max_samples=model.number_training_images,\n",
    "                                       test=False)\n",
    "# Initialize the data loader\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=1, \n",
    "                          shuffle=False,\n",
    "                          num_workers=8,\n",
    "                          persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de072c8",
   "metadata": {},
   "source": [
    "### 2.4 Other network settings\n",
    "\n",
    "We will finally set up a unique model name based on the network architecture so we can save and reload our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce79b3d3-ed51-4749-98b3-44ea2cfa45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('./src')\n",
    "sys.path.append('./models')\n",
    "sys.path.append('./settings')\n",
    "sys.path.append('./output')\n",
    "sys.path.append('./dataset')\n",
    "sys.path.append('./config')\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "import blitnet as bn\n",
    "import utils as ut\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from config import configure, image_csv, model_logger\n",
    "from dataset import CustomImageDataset, SetImageAsSpikes, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc786b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPRTempo78415685001.pth\n"
     ]
    }
   ],
   "source": [
    "def generate_model_name(model):\n",
    "    \"\"\"\n",
    "    Generate the model name based on its parameters.\n",
    "    \"\"\"\n",
    "    return (\"VPRTempo\" +\n",
    "            str(model.input) +\n",
    "            str(model.feature) +\n",
    "            str(model.output) +\n",
    "            str(model.number_modules) +\n",
    "            '.pth')\n",
    "\n",
    "model_name = generate_model_name(model)\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9e923809",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def model_logger(self):\n",
    "    \"\"\"\n",
    "    Log the model configuration to the console.\n",
    "    \"\"\"\n",
    "    model_logger(self)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3067711",
   "metadata": {},
   "source": [
    "### 2.5 Model quantization\n",
    "\n",
    "VPRTempoQuant makes use of Quantized Aware Training QAT and has a few simple steps to prepare the model to accomodate this. First, we will get the default quantization configuration for `fggbem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36244802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as quantization\n",
    "\n",
    "# Set the quantization configuration\n",
    "qconfig = quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a292cc",
   "metadata": {},
   "source": [
    "Next, we will set the model to be configured for network training and add our quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a884ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode and move to device\n",
    "model.train()\n",
    "model.to('cpu')\n",
    "model.qconfig = qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34b802",
   "metadata": {},
   "source": [
    "Now we will convert the model over to QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "087f3b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply quantization configurations to the model\n",
    "model = quantization.prepare_qat(model, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74ad2",
   "metadata": {},
   "source": [
    "At this point, we are ready to start training our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f4bdb",
   "metadata": {},
   "source": [
    "## 3. Set up and run the training \n",
    "\n",
    "### 3.1 Define the training regime\n",
    "\n",
    "Training through the layers is dynamic, such that all you need to do to train everything is to define a new layer. We will first define the training function and explain how it operates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb33796",
   "metadata": {},
   "source": [
    "This training regime runs every image through the newtork for a defined number of epochs. Images are loaded and converted to input spikes using the `train_loader` we defined earlier. For the first layer, it will simply calculate network spikes for the following layer. Otherwise, it will loop through each previous layer and generate spikes through the learned connection weights until it reaches the final layer currently being trained. \n",
    "\n",
    "The main calculation is in the `self.forward()` function. Weights in our layers are a `nn.Linear()` class, where input spikes are multiplied by connection weights between the layers. Once spikes from the input to layer have been calculated, spike-timing dependent plasticity (STDP) learning rules are applied. The learning rate for the network is annealed after every 100 images that are propagated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: feature_layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|████████████████████████████| 4000/4000 [01:25<00:00, 46.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: output_layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training :  67%|██████████████████▋         | 2671/4000 [00:54<00:26, 49.88it/s]"
     ]
    }
   ],
   "source": [
    "# Keep track of trained layers to pass data through them\n",
    "trained_layers = [] \n",
    "\n",
    "# Training each layer\n",
    "for layer_name, _ in sorted(model.layer_dict.items(), key=lambda item: item[1]):\n",
    "    print(f\"Training layer: {layer_name}\")\n",
    "    # Retrieve the layer object\n",
    "    layer = getattr(model, layer_name)\n",
    "    # Train the layer\n",
    "    model.train_model(train_loader, layer, prev_layers=trained_layers)\n",
    "    # After training the current layer, add it to the list of trained layers\n",
    "    trained_layers.append(layer_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51d8e6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
