{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70df0e83-9a35-41b3-81ec-cd12538045ed",
   "metadata": {},
   "source": [
    "## VPRTempo - Introduction\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Traditional methods for visual place recognition (VPR) tasks typically employ the use of convolutional neural networks like ResNet to train large datasets for feature extraction of incoming query images, rather than specifically learning said query place. The networks are extremely effective at accurate localisation, but are are slow to train, inference, and store.\n",
    "\n",
    "Spiking neural networks (SNNs) by contrast are more energy efficient and have low latency computation, meaning their deployment capability for VPR is extremely promising. Specifically, networks can be trained on the exact location you wish to query which takes a fundamentally different approach to the VPR task.\n",
    "\n",
    "VPRTempo uses a temporal encoding scheme for spikes, where the amplitude of a spike is determined by an incoming training or query image's pixel intensity. This amplitude defines the 'timing' of the spike, similar to a latency code. As spikes propagate throughout the system, spike-timing dependent plasticity (STDP) learning rules train neuronal connections based off of the pixel intensity spike amplitudes. \n",
    "\n",
    "To get started, please ensure you have installed and currently have activated the `conda` environment for VPRTempo. For more information how to install and setup the environment, please see the [README.md](https://github.com/AdamDHines/VPRTempo-quant/blob/main/README.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f846c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate vprtempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928d7a4",
   "metadata": {},
   "source": [
    "## 1. Get the Nordland dataset\n",
    "\n",
    "### 1.1 Download the dataset\n",
    "\n",
    "Please [download the Nordland datasets](https://webdiis.unizar.es/~jmfacil/pr-nordland/#download-dataset) (Summer, Spring, Fall, & Winter). There are two datasets available, the full size and downsampled versions. Either will work fine but our paper details the full size dataset. If disk space is a concern, please use the downsampled version.\n",
    "\n",
    "Save the data in the `./VPRTempo-quant/dataset/` subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a607d1",
   "metadata": {},
   "source": [
    "### 1.2 Import modules\n",
    "\n",
    "Once we have downloaded the dataset, we'll start by importing all the necessary modules.\n",
    "\n",
    "For this tutorial, we use [Jupyter Dynamic Classes](https://alexhagen.github.io/jdc/) so if not already installed please install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca1aa5e3-4537-4e1e-8629-bb134e749707",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../output')\n",
    "sys.path.append('../dataset')\n",
    "\n",
    "import blitnet as bn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from settings import configure, image_csv, model_logger\n",
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac2f0e",
   "metadata": {},
   "source": [
    "### 1.3 Prepare the dataset for the model (optional)\n",
    "\n",
    "The datset seasons are downloaded in .zip format and need to be extracted into a single folder. The `nordland` function has been provided to automatically do this for you and to re-name the images to match those in the nordland.csv file.\n",
    "\n",
    "If you have already done this from the previous tutorial, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f350d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "from nordland import nord_sort\n",
    "\n",
    "# unzip, re-organise, and re-name the Nordland datasets\n",
    "nord_sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2f885",
   "metadata": {},
   "source": [
    "## 2. Set up the network\n",
    "\n",
    "### 2.1 Define and initialize the VPRTempo model class\n",
    "\n",
    "We'll first define the VPRTempo class which handles the configuration as set in `./src/settings.py`, determining which images to load, and establishes the layers used for training. For this tutorial, leave the settings as the default.\n",
    "\n",
    "`__init__` is where we define the layers used for the model. In this case, we define a `feature_layer` and an `output_layer`. `dims` represents the number of neurons in the input and the layer itself, which in this case is `self.input`, `self.feature`, and `self.output`. Note that the size of the input for each proceeding layer is the size of previous layer. In this example, we have an input of 784 neurons (for 28x28 images) connected to a 1568 neuron feature layer which then connects to a final output layer of 500 neurons.\n",
    "\n",
    "The other hyperparameters for each layer are set here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPRTempo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VPRTempo, self).__init__()\n",
    "\n",
    "        # Configure the network\n",
    "        configure(self)\n",
    "        \n",
    "        # Define the images to load (both training and inference)\n",
    "        image_csv(self)\n",
    "\n",
    "        # Add quantization stubs for Quantization Aware Training (QAT)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        \n",
    "        # Define the add function for quantized addition\n",
    "        self.add = nn.quantized.FloatFunctional()      \n",
    "\n",
    "        # Layer dict to keep track of layer names and their order\n",
    "        self.layer_dict = {}\n",
    "        self.layer_counter = 0\n",
    "\n",
    "        \"\"\"\n",
    "        Define trainable layers here\n",
    "        \"\"\"\n",
    "        self.add_layer(\n",
    "            'feature_layer',\n",
    "            dims=[self.input, self.feature],\n",
    "            thr_range=[0, 0.5],\n",
    "            fire_rate=[0.2, 0.9],\n",
    "            ip_rate=0.15,\n",
    "            stdp_rate=0.005,\n",
    "            const_inp=[0, 0.1],\n",
    "            p=[0.1, 0.5]\n",
    "        )\n",
    "        self.add_layer(\n",
    "            'output_layer',\n",
    "            dims=[self.feature, self.output],\n",
    "            ip_rate=0.15,\n",
    "            stdp_rate=0.005,\n",
    "            spk_force=True\n",
    "        )\n",
    "        \n",
    "        print('VPRTempo succesfully initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e3c15b",
   "metadata": {},
   "source": [
    "### 2.2 Dynamically add layers\n",
    "\n",
    "As above, the only thing we need to do in order to add additional layers to our model is to include a self.add_layer(args) to the `__init__` component of the script. The actual handling of the layer generation is done by the blitnet.SNNLayer() class from `blitnet.py`. Here, hyperparameters are stored in the layer information and the initial weights are seeded and normalized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabd3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def add_layer(self, name, **kwargs):\n",
    "    \"\"\"\n",
    "    Dynamically add a layer with given name and keyword arguments.\n",
    "\n",
    "    :param name: Name of the layer to be added\n",
    "    :type name: str\n",
    "    :param kwargs: Hyperparameters for the layer\n",
    "    \"\"\"\n",
    "    # Check for layer name duplicates\n",
    "    if name in self.layer_dict:\n",
    "        raise ValueError(f\"Layer with name {name} already exists.\")\n",
    "\n",
    "    # Add a new SNNLayer with provided kwargs\n",
    "    setattr(self, name, bn.SNNLayer(**kwargs))\n",
    "\n",
    "    # Add layer name and index to the layer_dict\n",
    "    self.layer_dict[name] = self.layer_counter\n",
    "    self.layer_counter += 1  \n",
    "\n",
    "    print('Succesfully added '+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b92db1",
   "metadata": {},
   "source": [
    "### 2.3 Set the training regime\n",
    "\n",
    "Training is also handled by the `VPRTempo()` class and recursively runs until all the defined layers are trained. The initial learning rates are copied out so that they can be annealed appropriately for the defined number of time steps. Training runs for the specified number of epochs and the total number of timesteps as set in the train_loader class (more later on that, a simple [PyTorch DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)).\n",
    "\n",
    "Once a layer has been trained, the learning for that layer will be turned off and training deeper layers will propagate the input spikes through each trained layer until it reaches the one being currently learned. Learning involves spike-timing dependent plasticity (STDP) rules, firing threshold adjustments, and inhibitory connection normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "623595aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def train_model(self, train_loader, layer, prev_layers=None):\n",
    "    \"\"\"\n",
    "    Train a layer of the network model.\n",
    "\n",
    "    :param train_loader: Training data loader\n",
    "    :param layer: Layer to train\n",
    "    :param prev_layers: Previous layers to pass data through\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    pbar = tqdm(total=int(self.T * self.epoch),\n",
    "                desc=\"Training \",\n",
    "                position=0)\n",
    "\n",
    "    # Initialize the learning rates for each layer (used for annealment)\n",
    "    init_itp = layer.eta_ip.detach()\n",
    "    init_stdp = layer.eta_stdp.detach()\n",
    "\n",
    "    # Run training for the specified number of epochs\n",
    "    for epoch in range(self.epoch):\n",
    "        mod = 0  # Used to determine the learning rate annealment, resets at each epoch\n",
    "        # Run training for the specified number of timesteps\n",
    "        for spikes, labels in train_loader:\n",
    "            spikes, labels = spikes.to(self.device), labels.to(self.device)\n",
    "            idx = labels / self.filter # Set output index for spike forcing\n",
    "            # Pass through previous layers if they exist\n",
    "            if prev_layers:\n",
    "                with torch.no_grad():\n",
    "                    for prev_layer_name in prev_layers:\n",
    "                        prev_layer = getattr(self, prev_layer_name) # Get the previous layer object\n",
    "                        spikes = self.forward(spikes, prev_layer) # Pass spikes through the previous layer\n",
    "                        spikes = bn.clamp_spikes(spikes, prev_layer) # Clamp spikes [0, 0.9]\n",
    "            else:\n",
    "                prev_layer = None\n",
    "            # Get the output spikes from the current layer\n",
    "            pre_spike = spikes.detach() # Previous layer spikes for STDP\n",
    "            spikes = self.forward(spikes, layer) # Current layer spikes\n",
    "            spikes_noclp = spikes.detach() # Used for inhibitory homeostasis\n",
    "            spikes = bn.clamp_spikes(spikes, layer) # Clamp spikes [0, 0.9]\n",
    "            # Calculate STDP\n",
    "            layer = bn.calc_stdp(pre_spike,spikes,spikes_noclp,layer, idx, prev_layer=prev_layer)\n",
    "            # Adjust learning rates\n",
    "            layer = self._anneal_learning_rate(layer, mod, init_itp, init_stdp)\n",
    "            # Update the annealing mod & progress bar \n",
    "            mod += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e068e",
   "metadata": {},
   "source": [
    "### 2.4 Create the forward pass\n",
    "\n",
    "Layers in VPRTempo are defined as an [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer, with incoming spikes being linearly transformed with the layer weights. The forward pass simply takes incoming spikes and caluclates the transform with positive and negative weights and adds them together, returning the transformed spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a22d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def forward(self, spikes, layer):\n",
    "    \"\"\"\n",
    "    Compute the forward pass of the model.\n",
    "\n",
    "    Parameters:\n",
    "    - spikes (Tensor): Input spikes.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: Output after processing.\n",
    "    \"\"\"\n",
    "\n",
    "    spikes = self.quant(spikes)\n",
    "    spikes = self.add.add(layer.exc(spikes), layer.inh(spikes))\n",
    "    spikes = self.dequant(spikes)\n",
    "\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e16d",
   "metadata": {},
   "source": [
    "### 2.5 Learning rate annealment & model loader/saver\n",
    "\n",
    "Finally, the last thing we will add to the model is the learning rate annealment regime and the functions for loading and saving trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def _anneal_learning_rate(self, layer, mod, itp, stdp):\n",
    "    \"\"\"\n",
    "    Anneal the learning rate for the current layer.\n",
    "    \"\"\"\n",
    "    if np.mod(mod, 100) == 0: # Modify learning rate every 100 timesteps\n",
    "        pt = pow(float(self.T - mod) / self.T, self.annl_pow)\n",
    "        layer.eta_ip = torch.mul(itp, pt) # Anneal intrinsic threshold plasticity learning rate\n",
    "        layer.eta_stdp = torch.mul(stdp, pt) # Anneal STDP learning rate\n",
    "\n",
    "    return layer\n",
    "\n",
    "def save_model(self, model_out):    \n",
    "    \"\"\"\n",
    "    Save the trained model to models output folder.\n",
    "    \"\"\"\n",
    "    torch.save(self.state_dict(), model_out) \n",
    "\n",
    "def load_model(self, model_path):\n",
    "    \"\"\"\n",
    "    Load pre-trained model and set the state dictionary keys.\n",
    "    \"\"\"\n",
    "    self.load_state_dict(torch.load(model_path, map_location=self.device),\n",
    "                         strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d65918",
   "metadata": {},
   "source": [
    "### 2.6 Initialize the model\n",
    "\n",
    "Now that the model has been defined, we can initialize it and start with the quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55aa0e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = VPRTempo()\n",
    "model_logger(model)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d4a18",
   "metadata": {},
   "source": [
    "### 2.7 Generate unique model name\n",
    "\n",
    "We will finally set up a unique model name based on the network architecture so we can save and reload our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc786b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_name(model):\n",
    "    \"\"\"\n",
    "    Generate the model name based on its parameters.\n",
    "    \"\"\"\n",
    "    return (\"VPRTempo\" +\n",
    "            str(model.input) +\n",
    "            str(model.feature) +\n",
    "            str(model.output) +\n",
    "            str(model.number_modules) +\n",
    "            '.pth')\n",
    "\n",
    "model_name = generate_model_name(model)\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17640d20",
   "metadata": {},
   "source": [
    "## 3. Define the DataLoader\n",
    "\n",
    "### 3.1 Set the DataLoader\n",
    "\n",
    "Now that we've defined the model, we will set up the DataLoaders. These utilise a PyTorch CustomImageDataset and ProcessImage to import images and process them for training or inference. In brief, images are loaded, gamma corrected, resized, and then patch-normalized before being converted into system spikes to be propagated throughout.\n",
    "\n",
    "Since we present the network with one image at a time, the `batch_size` is kept to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "train_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                       img_dirs=model.training_dirs,\n",
    "                                       transform=image_transform,\n",
    "                                       skip=model.filter,\n",
    "                                       max_samples=model.number_training_images,\n",
    "                                       test=False)\n",
    "# Initialize the data loader\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=1, \n",
    "                          shuffle=False,\n",
    "                          num_workers=8,\n",
    "                          persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f4bdb",
   "metadata": {},
   "source": [
    "## 5. Set up and run the training \n",
    "\n",
    "### 5.1 Define and run the training regime\n",
    "\n",
    "The training will loop through each defined layer until every single one has trained. In order to propagate spikes throughout the system, trained layers are appended to a list so that they can be re-fed back into the network to calculate spikes based on learned weights.\n",
    "\n",
    "Run the below cell to train our `feature_layer` and `output_layer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of trained layers to pass data through them\n",
    "trained_layers = [] \n",
    "\n",
    "# Training each layer\n",
    "for layer_name, _ in sorted(model.layer_dict.items(), key=lambda item: item[1]):\n",
    "    print(f\"Training layer: {layer_name}\")\n",
    "    # Retrieve the layer object\n",
    "    layer = getattr(model, layer_name)\n",
    "    # Train the layer\n",
    "    model.train_model(train_loader, layer, prev_layers=trained_layers)\n",
    "    # After training the current layer, add it to the list of trained layers\n",
    "    trained_layers.append(layer_name)\n",
    "    \n",
    "print('All layers trained succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6daea3",
   "metadata": {},
   "source": [
    "### 5.2 Convert and save the model\n",
    "\n",
    "Now that the training has been completed, we can convert the QAT model over to be fully quantized. As the layers were trained, scale and zero-point factors will learned for all the elements of the model and can now be applied to the layers. Once converted, we will save the model for use in inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ededaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to eval mode\n",
    "model.eval()\n",
    "# Save the model\n",
    "model.save_model(os.path.join('../models', model_name))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d69843",
   "metadata": {},
   "source": [
    "## 6. Inferencing\n",
    "\n",
    "As in the previous tutorial, inferencing with a trained model is quite simple. The only additional thing we need to do is reinitialize the VPRTempo class and convert it to quantized before loading the model. Without pre-quantizing the inference model, state dictionary keys will not match since all the layers and associated components have new parameters such as scale and zero-point.\n",
    "\n",
    "### 6.1 Add the inference function to the VPRTempo class\n",
    "\n",
    "We will start by adding in the inference function to VPRTempo. It is similar to the training regime but omits the learning components `calc_stdp` and simply runs through all the layers until it reaches the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a368c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def evaluate(self, model, test_loader, layers=None):\n",
    "    \"\"\"\n",
    "    Run the inferencing model and calculate the accuracy.\n",
    "\n",
    "    :param test_loader: Testing data loader\n",
    "    :param layers: Layers to pass data through\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the number of correct predictions\n",
    "    numcorr = 0\n",
    "    idx = 0\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    pbar = tqdm(total=self.number_testing_images,\n",
    "                desc=\"Running the test network\",\n",
    "                position=0)\n",
    "\n",
    "    # Run inference for the specified number of timesteps\n",
    "    for spikes, labels in test_loader:\n",
    "        # Set device\n",
    "        spikes, labels = spikes.to(self.device), labels.to(self.device)\n",
    "        # Pass through previous layers if they exist\n",
    "        if layers:\n",
    "            for layer_name in layers:\n",
    "                layer = getattr(self, layer_name)\n",
    "                spikes = self.forward(spikes, layer)\n",
    "                spikes = bn.clamp_spikes(spikes, layer)\n",
    "\n",
    "        # Evaluate if the prediction is correct\n",
    "        if torch.argmax(spikes.reshape(1, self.number_training_images)) == idx:\n",
    "            numcorr += 1\n",
    "\n",
    "        # Update the index and progress bar\n",
    "        idx += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    pbar.close()\n",
    "    # Calculate and record the accuracy\n",
    "    accuracy = round((numcorr/self.number_testing_images)*100,2)\n",
    "    model.logger.info(\"P@100R: \"+ str(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e841fc7",
   "metadata": {},
   "source": [
    "### 6.2 Define the inferencing DataLoader\n",
    "\n",
    "The only difference between the training and testing DataLoader is the directory with which it will import images from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the image transforms and datasets\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "test_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                  img_dirs=model.testing_dirs,\n",
    "                                  transform=image_transform,\n",
    "                                  skip=model.filter,\n",
    "                                  max_samples=model.number_testing_images)\n",
    "# Initialize the data loader\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False,\n",
    "                         num_workers=8,\n",
    "                         persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018de09a",
   "metadata": {},
   "source": [
    "### 6.3 Re-initialize the model class, convert to quantization, and load the model\n",
    "\n",
    "Now we will re-initialize the VPRTempo class model, set to eval mode, and convert it over to quantized so that we can import our newly trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d51e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode and set configuration\n",
    "model = VPRTempo()\n",
    "model.model_logger()\n",
    "model.eval()\n",
    "\n",
    "# Load the model\n",
    "model.load_model(os.path.join('../models', model_name))\n",
    "\n",
    "# Retrieve layer names for inference\n",
    "layer_names = list(model.layer_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d24e8",
   "metadata": {},
   "source": [
    "### 6.4 Run the model inference\n",
    "\n",
    "Now we are ready to inference the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use evaluate method for inference accuracy\n",
    "model.evaluate(model, test_loader, layers=layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f46e43",
   "metadata": {},
   "source": [
    "## 7. Conslusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e86c5",
   "metadata": {},
   "source": [
    "This tutorial covered how we can convert the VPRTempo model to perform Quantized Aware Training (QAT) to keep the model size more lightweight. You might notice that if you compare the system between FP32 to Int8, the model works equally as well with a reduced bit-depth with the added benefit of a reduced model size.\n",
    "\n",
    "To read more about QAT and quantization in general, PyTorch provides many useful articles;\n",
    "https://pytorch.org/docs/stable/quantization.html\n",
    "https://pytorch.org/blog/quantization-in-practice/\n",
    "\n",
    "The key benefit to this is being able to perform fast training and inferencing on CPU architecture, which for resource limited compute scenarios is critical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
