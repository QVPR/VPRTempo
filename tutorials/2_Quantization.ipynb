{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70df0e83-9a35-41b3-81ec-cd12538045ed",
   "metadata": {},
   "source": [
    "## VPRTempo - Quantized Aware Training and Inferencing Tutorial\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this tutorial, we are going to take the base VPRTempo model to train and inference a network with PyTorch's Quantized Aware Training ([QAT](https://pytorch.org/docs/stable/quantization.html)). Functionally, this tutorial is similar to the previous one but will be simplified. For a more detailed dive into how VPRTempo works, please see [Tutorial 1](https://github.com/AdamDHines/VPRTempo-quant/blob/main/tutorials/1_Introduction.ipynb)\n",
    "\n",
    "**Note: it does not appear that Apple Silicon is currently a supported backend for QAT**\n",
    "\n",
    "To get started, please ensure you have installed and currently have activated the `conda` environment for VPRTempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f846c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate vprtempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928d7a4",
   "metadata": {},
   "source": [
    "## 1. Get the Nordland dataset\n",
    "\n",
    "### 1.1 Download the dataset\n",
    "\n",
    "Please [download the Nordland datasets](https://webdiis.unizar.es/~jmfacil/pr-nordland/#download-dataset) (Summer, Spring, Fall, & Winter). There are two datasets available, the full size and downsampled versions. Either will work fine but our paper details the full size dataset. If disk space is a concern, please use the downsampled version.\n",
    "\n",
    "Save the data in the `./VPRTempo-quant/dataset/` subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a607d1",
   "metadata": {},
   "source": [
    "### 1.2 Import modules\n",
    "\n",
    "Once we have downloaded the dataset, we'll start by importing all the necessary modules.\n",
    "\n",
    "For this tutorial, we use [Jupyter Dynamic Classes](https://alexhagen.github.io/jdc/) so if not already installed please install. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf4ad2-755b-40f1-8bd3-5b98376ed5df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install jdc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9caff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../output')\n",
    "sys.path.append('../dataset')\n",
    "\n",
    "import blitnet as bn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from settings import configure, image_csv, model_logger\n",
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac2f0e",
   "metadata": {},
   "source": [
    "### 1.3 Prepare the dataset for the model (optional)\n",
    "\n",
    "The datset seasons are downloaded in .zip format and need to be extracted into a single folder. The `nordland` function has been provided to automatically do this for you and to re-name the images to match those in the nordland.csv file.\n",
    "\n",
    "If you have already done this from the previous tutorial, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f350d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "from nordland import nord_sort\n",
    "\n",
    "# unzip, re-organise, and re-name the Nordland datasets\n",
    "nord_sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2f885",
   "metadata": {},
   "source": [
    "## 2. Set up the network\n",
    "\n",
    "### 2.1 Define and initialize the VPRTempo model class\n",
    "\n",
    "We'll now import the main network model class `VPRTempo`. Please see [Tutorial 1](https://github.com/AdamDHines/VPRTempo-quant/blob/main/tutorials/1_Introduction.ipynb) for a more detailed look at what this includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b5130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VPRTempo import VPRTempo\n",
    "model = VPRTempo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d4a18",
   "metadata": {},
   "source": [
    "### 2.2 Generate unique model name\n",
    "\n",
    "We will set up a unique model name to save and load for inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc786b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_model_name(model):\n",
    "    \"\"\"\n",
    "    Generate the model name based on its parameters.\n",
    "    \"\"\"\n",
    "    return (\"VPRTempo\" +\n",
    "            str(model.input) +\n",
    "            str(model.feature) +\n",
    "            str(model.output) +\n",
    "            str(model.number_modules) +\n",
    "            \"Quantized\"+\n",
    "            '.pth')\n",
    "\n",
    "model_name = generate_model_name(model)\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17640d20",
   "metadata": {},
   "source": [
    "## 3. Define the DataLoader\n",
    "\n",
    "### 3.1 Set the DataLoader\n",
    "\n",
    "Now that we've defined the model, we will set up the DataLoaders. These utilise a PyTorch CustomImageDataset and ProcessImage to import images and process them for training or inference. In brief, images are loaded, gamma corrected, resized, and then patch-normalized before being converted into system spikes to be propagated throughout.\n",
    "\n",
    "Since we present the network with one image at a time, the `batch_size` is kept to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "train_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                       img_dirs=model.training_dirs,\n",
    "                                       transform=image_transform,\n",
    "                                       skip=model.filter,\n",
    "                                       max_samples=model.number_training_images,\n",
    "                                       test=False)\n",
    "# Initialize the data loader\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=1, \n",
    "                          shuffle=False,\n",
    "                          num_workers=8,\n",
    "                          persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3067711",
   "metadata": {},
   "source": [
    "## 4. Quantization\n",
    "\n",
    "### 4.1 Model quantization\n",
    "\n",
    "VPRTempoQuant makes use of Quantized Aware Training QAT and has a few simple steps to prepare the model to accomodate this. First, we will get the default quantization configuration for `fggbem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36244802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as quantization\n",
    "\n",
    "# Set the quantization configuration\n",
    "qconfig = quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a292cc",
   "metadata": {},
   "source": [
    "Next, we will set the model to be configured for network training and add our quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a884ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode and move to device\n",
    "model.train()\n",
    "model.to('cpu')\n",
    "model.qconfig = qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34b802",
   "metadata": {},
   "source": [
    "Now we will convert the model over to QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087f3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantization configurations to the model\n",
    "model = quantization.prepare_qat(model, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74ad2",
   "metadata": {},
   "source": [
    "At this point, we are ready to start training our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f4bdb",
   "metadata": {},
   "source": [
    "## 5. Set up and run the training \n",
    "\n",
    "### 5.1 Define and run the training regime\n",
    "\n",
    "The training will loop through each defined layer until every single one has trained. In order to propagate spikes throughout the system, trained layers are appended to a list so that they can be re-fed back into the network to calculate spikes based on learned weights.\n",
    "\n",
    "Run the below cell to train our `feature_layer` and `output_layer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0075638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep track of trained layers to pass data through them\n",
    "trained_layers = [] \n",
    "\n",
    "# Training each layer\n",
    "for layer_name, _ in sorted(model.layer_dict.items(), key=lambda item: item[1]):\n",
    "    print(f\"Training layer: {layer_name}\")\n",
    "    # Retrieve the layer object\n",
    "    layer = getattr(model, layer_name)\n",
    "    # Train the layer\n",
    "    model.train_model(train_loader, layer, prev_layers=trained_layers)\n",
    "    # After training the current layer, add it to the list of trained layers\n",
    "    trained_layers.append(layer_name)\n",
    "    \n",
    "print('All layers trained succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6daea3",
   "metadata": {},
   "source": [
    "### 5.2 Convert and save the model\n",
    "\n",
    "Now that the training has been completed, we can convert the QAT model over to be fully quantized. As the layers were trained, scale and zero-point factors will learned for all the elements of the model and can now be applied to the layers. Once converted, we will save the model for use in inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ededaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the model to a quantized model\n",
    "model = quantization.convert(model, inplace=False)\n",
    "model.eval()\n",
    "# Save the model\n",
    "model.save_model(os.path.join('../models', model_name))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018de09a",
   "metadata": {},
   "source": [
    "### 6.3 Re-initialize the model class, convert to quantization, and load the model\n",
    "\n",
    "Now we will re-initialize the VPRTempo class model, set to eval mode, and convert it over to quantized so that we can import our newly trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d51e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode and set configuration\n",
    "model = VPRTempo()\n",
    "model.model_logger()\n",
    "model.eval()\n",
    "model.qconfig = qconfig\n",
    "\n",
    "# Apply quantization configurations to all layers in layer_dict\n",
    "for layer_name, _ in model.layer_dict.items():\n",
    "    getattr(model, layer_name).qconfig = qconfig\n",
    "# Prepare and convert the model to a quantized model\n",
    "model = quantization.prepare(model, inplace=False)\n",
    "model = quantization.convert(model, inplace=False)\n",
    "# Load the model\n",
    "model.load_model(os.path.join('../models', model_name))\n",
    "\n",
    "# Retrieve layer names for inference\n",
    "layer_names = list(model.layer_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e841fc7",
   "metadata": {},
   "source": [
    "### 6.2 Define the inferencing DataLoader\n",
    "\n",
    "The only difference between the training and testing DataLoader is the directory with which it will import images from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the image transforms and datasets\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "test_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                  img_dirs=model.testing_dirs,\n",
    "                                  transform=image_transform,\n",
    "                                  skip=model.filter,\n",
    "                                  max_samples=model.number_testing_images)\n",
    "# Initialize the data loader\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False,\n",
    "                         num_workers=8,\n",
    "                         persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d24e8",
   "metadata": {},
   "source": [
    "### 6.4 Run the model inference\n",
    "\n",
    "Now we are ready to inference the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use evaluate method for inference accuracy\n",
    "model.evaluate(model, test_loader, layers=layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f46e43",
   "metadata": {},
   "source": [
    "## 7. Conslusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e86c5",
   "metadata": {},
   "source": [
    "This tutorial covered how we can convert the VPRTempo model to perform Quantized Aware Training (QAT) to keep the model size more lightweight. You might notice that if you compare the system between FP32 to Int8, the model works equally as well with a reduced bit-depth with the added benefit of a reduced model size.\n",
    "\n",
    "To read more about QAT and quantization in general, PyTorch provides many useful articles;\n",
    "https://pytorch.org/docs/stable/quantization.html\n",
    "https://pytorch.org/blog/quantization-in-practice/\n",
    "\n",
    "The key benefit to this is being able to perform fast training and inferencing on CPU architecture, which for resource limited compute scenarios is critical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
