{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70df0e83-9a35-41b3-81ec-cd12538045ed",
   "metadata": {},
   "source": [
    "## VPRTempo - Quantized Aware Training and Inferencing Tutorial\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this tutorial, we are going to take the base VPRTempo model to train and inference a network with PyTorch's Quantized Aware Training ([QAT](https://pytorch.org/docs/stable/quantization.html)). Functionally, this tutorial is similar to the previous one but will be simplified. For a more detailed dive into how VPRTempo works, please see [Tutorial 1](https://github.com/AdamDHines/VPRTempo-quant/blob/main/tutorials/1_Introduction.ipynb)\n",
    "\n",
    "**Note: it does not appear that Apple Silicon is currently a supported backend for QAT**\n",
    "\n",
    "To get started, please ensure you have installed and currently have activated the `conda` environment for VPRTempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f846c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate vprtempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928d7a4",
   "metadata": {},
   "source": [
    "## 1. Get the Nordland dataset\n",
    "\n",
    "### 1.1 Download the dataset\n",
    "\n",
    "Please [download the Nordland datasets](https://webdiis.unizar.es/~jmfacil/pr-nordland/#download-dataset) (Summer, Spring, Fall, & Winter). There are two datasets available, the full size and downsampled versions. Either will work fine but our paper details the full size dataset. If disk space is a concern, please use the downsampled version.\n",
    "\n",
    "Save the data in the `./VPRTempo-quant/dataset/` subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a607d1",
   "metadata": {},
   "source": [
    "### 1.2 Import modules\n",
    "\n",
    "Once we have downloaded the dataset, we'll start by importing all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9caff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../output')\n",
    "sys.path.append('../dataset')\n",
    "\n",
    "import blitnet as bn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from settings import configure, image_csv, model_logger\n",
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac2f0e",
   "metadata": {},
   "source": [
    "### 1.3 Prepare the dataset for the model (optional)\n",
    "\n",
    "The datset seasons are downloaded in .zip format and need to be extracted into a single folder. The `nordland` function has been provided to automatically do this for you and to re-name the images to match those in the nordland.csv file.\n",
    "\n",
    "If you have already done this from the previous tutorial, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f350d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "from nordland import nord_sort\n",
    "\n",
    "# unzip, re-organise, and re-name the Nordland datasets\n",
    "nord_sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2f885",
   "metadata": {},
   "source": [
    "## 2. Set up the network\n",
    "\n",
    "### 2.1 Define and initialize the VPRTempo model class\n",
    "\n",
    "We'll now import the main network model class `VPRTempo`. Please see [Tutorial 1](https://github.com/AdamDHines/VPRTempo-quant/blob/main/tutorials/1_Introduction.ipynb) for a more detailed look at what this includes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99b5130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from VPRTempo import VPRTempo\n",
    "model = VPRTempo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d4a18",
   "metadata": {},
   "source": [
    "### 2.2 Generate unique model name\n",
    "\n",
    "We will set up a unique model name to save and load for inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dc786b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPRTempo78415685001Quantized.pth\n"
     ]
    }
   ],
   "source": [
    "def generate_model_name(model):\n",
    "    \"\"\"\n",
    "    Generate the model name based on its parameters.\n",
    "    \"\"\"\n",
    "    return (\"VPRTempo\" +\n",
    "            str(model.input) +\n",
    "            str(model.feature) +\n",
    "            str(model.output) +\n",
    "            str(model.number_modules) +\n",
    "            \"Quantized\"+\n",
    "            '.pth')\n",
    "\n",
    "model_name = generate_model_name(model)\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17640d20",
   "metadata": {},
   "source": [
    "## 3. Define the DataLoader\n",
    "\n",
    "### 3.1 Set the DataLoader\n",
    "\n",
    "Now that we've defined the model, we will set up the DataLoaders. These utilise a PyTorch CustomImageDataset and ProcessImage to import images and process them for training or inference. In brief, images are loaded, gamma corrected, resized, and then patch-normalized before being converted into system spikes to be propagated throughout.\n",
    "\n",
    "Since we present the network with one image at a time, the `batch_size` is kept to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6714bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "train_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                       img_dirs=model.training_dirs,\n",
    "                                       transform=image_transform,\n",
    "                                       skip=model.filter,\n",
    "                                       max_samples=model.number_training_images,\n",
    "                                       test=False)\n",
    "# Initialize the data loader\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=1, \n",
    "                          shuffle=False,\n",
    "                          num_workers=8,\n",
    "                          persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3067711",
   "metadata": {},
   "source": [
    "## 4. Quantization\n",
    "\n",
    "### 4.1 Model quantization\n",
    "\n",
    "VPRTempoQuant makes use of Quantized Aware Training QAT and has a few simple steps to prepare the model to accomodate this. First, we will get the default quantization configuration for `fggbem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "36244802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as quantization\n",
    "\n",
    "# Set the quantization configuration\n",
    "qconfig = quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a292cc",
   "metadata": {},
   "source": [
    "Next, we will set the model to be configured for network training and add our quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a884ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode and move to device\n",
    "model.train()\n",
    "model.to('cpu')\n",
    "model.qconfig = qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34b802",
   "metadata": {},
   "source": [
    "Now we will convert the model over to QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "087f3b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply quantization configurations to the model\n",
    "model = quantization.prepare_qat(model, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74ad2",
   "metadata": {},
   "source": [
    "At this point, we are ready to start training our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f4bdb",
   "metadata": {},
   "source": [
    "## 5. Set up and run the training \n",
    "\n",
    "### 5.1 Define and run the training regime\n",
    "\n",
    "The training will loop through each defined layer until every single one has trained. In order to propagate spikes throughout the system, trained layers are appended to a list so that they can be re-fed back into the network to calculate spikes based on learned weights.\n",
    "\n",
    "Run the below cell to train our `feature_layer` and `output_layer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0075638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: feature_layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|████████████████████████████| 4000/4000 [01:28<00:00, 45.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: output_layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|████████████████████████████| 4000/4000 [01:20<00:00, 49.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All layers trained succesfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Keep track of trained layers to pass data through them\n",
    "trained_layers = [] \n",
    "\n",
    "# Training each layer\n",
    "for layer_name, _ in sorted(model.layer_dict.items(), key=lambda item: item[1]):\n",
    "    print(f\"Training layer: {layer_name}\")\n",
    "    # Retrieve the layer object\n",
    "    layer = getattr(model, layer_name)\n",
    "    # Train the layer\n",
    "    model.train_model(train_loader, layer, prev_layers=trained_layers)\n",
    "    # After training the current layer, add it to the list of trained layers\n",
    "    trained_layers.append(layer_name)\n",
    "    \n",
    "print('All layers trained succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6daea3",
   "metadata": {},
   "source": [
    "### 5.2 Convert and save the model\n",
    "\n",
    "Now that the training has been completed, we can convert the QAT model over to be fully quantized. As the layers were trained, scale and zero-point factors will learned for all the elements of the model and can now be applied to the layers. Once converted, we will save the model for use in inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "53ededaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the model to a quantized model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m quantization\u001b[38;5;241m.\u001b[39mconvert(model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:551\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    550\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 551\u001b[0m _convert(\n\u001b[1;32m    552\u001b[0m     module, mapping, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_reference\u001b[38;5;241m=\u001b[39mis_reference,\n\u001b[1;32m    553\u001b[0m     convert_custom_config_dict\u001b[38;5;241m=\u001b[39mconvert_custom_config_dict)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    555\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:589\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 589\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    590\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m    591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    589\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    590\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    594\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:624\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    622\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n\u001b[1;32m    625\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:277\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeight observer must have dtype torch.qint8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    276\u001b[0m qweight \u001b[38;5;241m=\u001b[39m _quantize_weight(mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat(), weight_post_process)\n\u001b[0;32m--> 277\u001b[0m qlinear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(mod\u001b[38;5;241m.\u001b[39min_features,\n\u001b[1;32m    278\u001b[0m               mod\u001b[38;5;241m.\u001b[39mout_features,\n\u001b[1;32m    279\u001b[0m               dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    280\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, mod\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    281\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(act_scale)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:151\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias_, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported dtype specified for quantized Linear!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m LinearPackedParams(dtype)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, bias)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:27\u001b[0m, in \u001b[0;36mLinearPackedParams.__init__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     26\u001b[0m     wq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight_bias(wq, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:32\u001b[0m, in \u001b[0;36mLinearPackedParams.set_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch\u001b[38;5;241m.\u001b[39mTensor, bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack(weight, bias)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack_fp16(weight, bias)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "# Convert the model to a quantized model\n",
    "model = quantization.convert(model, inplace=False)\n",
    "model.eval()\n",
    "# Save the model\n",
    "model.save_model(os.path.join('./models', model_name))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e841fc7",
   "metadata": {},
   "source": [
    "### 6.2 Define the inferencing DataLoader\n",
    "\n",
    "The only difference between the training and testing DataLoader is the directory with which it will import images from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a1adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the image transforms and datasets\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "test_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                  img_dirs=model.testing_dirs,\n",
    "                                  transform=image_transform,\n",
    "                                  skip=model.filter,\n",
    "                                  max_samples=model.number_testing_images)\n",
    "# Initialize the data loader\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False,\n",
    "                         num_workers=8,\n",
    "                         persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018de09a",
   "metadata": {},
   "source": [
    "### 6.3 Re-initialize the model class, convert to quantization, and load the model\n",
    "\n",
    "Now we will re-initialize the VPRTempo class model, set to eval mode, and convert it over to quantized so that we can import our newly trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d51e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to evaluation mode and set configuration\n",
    "model = VPRTempo()\n",
    "model.eval()\n",
    "model.qconfig = qconfig\n",
    "\n",
    "# Apply quantization configurations to all layers in layer_dict\n",
    "for layer_name, _ in model.layer_dict.items():\n",
    "    getattr(model, layer_name).qconfig = qconfig\n",
    "# Prepare and convert the model to a quantized model\n",
    "model = quantization.prepare(model, inplace=False)\n",
    "model = quantization.convert(model, inplace=False)\n",
    "# Load the model\n",
    "model.load_model(os.path.join('./models', model_name))\n",
    "\n",
    "# Retrieve layer names for inference\n",
    "layer_names = list(model.layer_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d24e8",
   "metadata": {},
   "source": [
    "### 6.4 Run the model inference\n",
    "\n",
    "Now we are ready to inference the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37aa84e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use evaluate method for inference accuracy\n",
    "model.evaluate(test_loader, layers=layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f46e43",
   "metadata": {},
   "source": [
    "## 7. Conslusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e86c5",
   "metadata": {},
   "source": [
    "This tutorial covered how we can convert the VPRTempo model to perform Quantized Aware Training (QAT) to keep the model size more lightweight. You might notice that if you compare the system between FP32 to Int8, the model works equally as well with a reduced bit-depth with the added benefit of a reduced model size.\n",
    "\n",
    "To read more about QAT and quantization in general, PyTorch provides many useful articles;\n",
    "https://pytorch.org/docs/stable/quantization.html\n",
    "https://pytorch.org/blog/quantization-in-practice/\n",
    "\n",
    "The key benefit to this is being able to perform fast training and inferencing on CPU architecture, which for resource limited compute scenarios is critical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
