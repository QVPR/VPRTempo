{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70df0e83-9a35-41b3-81ec-cd12538045ed",
   "metadata": {},
   "source": [
    "## VPRTempo - Quantized Aware Training and Inferencing Tutorial\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "Traditional methods for visual place recognition (VPR) tasks typically employ the use of convolutional neural networks like ResNet to train large datasets for feature extraction of incoming query images, rather than specifically learning said query place. The networks are extremely effective at accurate localisation, but are are slow to train, inference, and store.\n",
    "\n",
    "Spiking neural networks (SNNs) by contrast are more energy efficient and have low latency computation, meaning their deployment capability for VPR is extremely promising. Specifically, networks can be trained on the exact location you wish to query which takes a fundamentally different approach to the VPR task.\n",
    "\n",
    "VPRTempo uses a temporal encoding scheme for spikes, where the amplitude of a spike is determined by an incoming training or query image's pixel intensity. This amplitude defines the 'timing' of the spike, similar to a latency code. As spikes propagate throughout the system, spike-timing dependent plasticity (STDP) learning rules train neuronal connections based off of the pixel intensity spike amplitudes. \n",
    "\n",
    "In this tutorial, we are going to take the base VPRTempo model to train and inference a network with PyTorch's Quantized Aware Training ([QAT](https://pytorch.org/docs/stable/quantization.html)). Functionally, this tutorial is similar to the previous one as we go through and define the network architecture so if you can skip to **4. Quantization** if you are already familiar with how this works.\n",
    "\n",
    "**Note: it does not appear that Apple Silicon is currently a supported backend for QAT**\n",
    "\n",
    "To get started, please ensure you have installed and currently have activated the `conda` environment for VPRTempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f846c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "!conda activate vprtempo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0928d7a4",
   "metadata": {},
   "source": [
    "## 1. Get the Nordland dataset\n",
    "\n",
    "### 1.1 Download the dataset\n",
    "\n",
    "Please [download the Nordland datasets](https://webdiis.unizar.es/~jmfacil/pr-nordland/#download-dataset) (Summer, Spring, Fall, & Winter). There are two datasets available, the full size and downsampled versions. Either will work fine but our paper details the full size dataset. If disk space is a concern, please use the downsampled version.\n",
    "\n",
    "Save the data in the `./VPRTempo-quant/dataset/` subfolder."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0a607d1",
   "metadata": {},
   "source": [
    "### 1.2 Import modules\n",
    "\n",
    "Once we have downloaded the dataset, we'll start by importing all the necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9caff25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jdc\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../models')\n",
    "sys.path.append('../output')\n",
    "sys.path.append('../dataset')\n",
    "\n",
    "import blitnet as bn\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.quantization as quantization\n",
    "\n",
    "from settings import configure, image_csv, model_logger\n",
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.ao.quantization import QuantStub, DeQuantStub\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffac2f0e",
   "metadata": {},
   "source": [
    "### 1.3 Prepare the dataset for the model (optional)\n",
    "\n",
    "The datset seasons are downloaded in .zip format and need to be extracted into a single folder. The `nordland` function has been provided to automatically do this for you and to re-name the images to match those in the nordland.csv file.\n",
    "\n",
    "If you have already done this from the previous tutorial, you can skip this step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f350d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from os import walk\n",
    "from nordland import nord_sort\n",
    "\n",
    "# unzip, re-organise, and re-name the Nordland datasets\n",
    "nord_sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d2f885",
   "metadata": {},
   "source": [
    "## 2. Set up the network\n",
    "\n",
    "### 2.1 Define and initialize the VPRTempo model class\n",
    "\n",
    "We'll first define the VPRTempo class which handles the configuration as set in `./src/settings.py`, determining which images to load, and establishes the layers used for training. For this tutorial, leave the settings as the default.\n",
    "\n",
    "`__init__` is where we define the layers used for the model. In this case, we define a `feature_layer` and an `output_layer`. `dims` represents the number of neurons in the input and the layer itself, which in this case is `self.input`, `self.feature`, and `self.output`. Note that the size of the input for each proceeding layer is the size of previous layer. In this example, we have an input of 784 neurons (for 28x28 images) connected to a 1568 neuron feature layer which then connects to a final output layer of 500 neurons.\n",
    "\n",
    "The other hyperparameters for each layer are set here as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "99b5130c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VPRTempo(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VPRTempo, self).__init__()\n",
    "\n",
    "        # Configure the network\n",
    "        configure(self)\n",
    "        \n",
    "        # Define the images to load (both training and inference)\n",
    "        image_csv(self)\n",
    "\n",
    "        # Add quantization stubs for Quantization Aware Training (QAT)\n",
    "        self.quant = QuantStub()\n",
    "        self.dequant = DeQuantStub()\n",
    "        \n",
    "        # Define the add function for quantized addition\n",
    "        self.add = nn.quantized.FloatFunctional()      \n",
    "\n",
    "        # Layer dict to keep track of layer names and their order\n",
    "        self.layer_dict = {}\n",
    "        self.layer_counter = 0\n",
    "\n",
    "        \"\"\"\n",
    "        Define trainable layers here\n",
    "        \"\"\"\n",
    "        self.add_layer(\n",
    "            'feature_layer',\n",
    "            dims=[self.input, self.feature],\n",
    "            thr_range=[0, 0.5],\n",
    "            fire_rate=[0.2, 0.9],\n",
    "            ip_rate=0.15,\n",
    "            stdp_rate=0.005,\n",
    "            const_inp=[0, 0.1],\n",
    "            p=[0.1, 0.5]\n",
    "        )\n",
    "        self.add_layer(\n",
    "            'output_layer',\n",
    "            dims=[self.feature, self.output],\n",
    "            ip_rate=0.15,\n",
    "            stdp_rate=0.005,\n",
    "            spk_force=True\n",
    "        )\n",
    "        \n",
    "        print('VPRTempo succesfully initialized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9e3c15b",
   "metadata": {},
   "source": [
    "### 2.2 Dynamically add layers\n",
    "\n",
    "As above, the only thing we need to do in order to add additional layers to our model is to include a self.add_layer(args) to the `__init__` component of the script. The actual handling of the layer generation is done by the blitnet.SNNLayer() class from `blitnet.py`. Here, hyperparameters are stored in the layer information and the initial weights are seeded and normalized for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dabd3c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def add_layer(self, name, **kwargs):\n",
    "    \"\"\"\n",
    "    Dynamically add a layer with given name and keyword arguments.\n",
    "\n",
    "    :param name: Name of the layer to be added\n",
    "    :type name: str\n",
    "    :param kwargs: Hyperparameters for the layer\n",
    "    \"\"\"\n",
    "    # Check for layer name duplicates\n",
    "    if name in self.layer_dict:\n",
    "        raise ValueError(f\"Layer with name {name} already exists.\")\n",
    "\n",
    "    # Add a new SNNLayer with provided kwargs\n",
    "    setattr(self, name, bn.SNNLayer(**kwargs))\n",
    "\n",
    "    # Add layer name and index to the layer_dict\n",
    "    self.layer_dict[name] = self.layer_counter\n",
    "    self.layer_counter += 1  \n",
    "\n",
    "    print('Succesfully added '+name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3b92db1",
   "metadata": {},
   "source": [
    "### 2.3 Set the training regime\n",
    "\n",
    "Training is also handled by the `VPRTempo()` class and recursively runs until all the defined layers are trained. The initial learning rates are copied out so that they can be annealed appropriately for the defined number of time steps. Training runs for the specified number of epochs and the total number of timesteps as set in the train_loader class (more later on that, a simple [PyTorch DataLoader](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)).\n",
    "\n",
    "Once a layer has been trained, the learning for that layer will be turned off and training deeper layers will propagate the input spikes through each trained layer until it reaches the one being currently learned. Learning involves spike-timing dependent plasticity (STDP) rules, firing threshold adjustments, and inhibitory connection normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "623595aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def train_model(self, train_loader, layer, prev_layers=None):\n",
    "    \"\"\"\n",
    "    Train a layer of the network model.\n",
    "\n",
    "    :param train_loader: Training data loader\n",
    "    :param layer: Layer to train\n",
    "    :param prev_layers: Previous layers to pass data through\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    pbar = tqdm(total=int(self.T * self.epoch),\n",
    "                desc=\"Training \",\n",
    "                position=0)\n",
    "\n",
    "    # Initialize the learning rates for each layer (used for annealment)\n",
    "    init_itp = layer.eta_ip.detach()\n",
    "    init_stdp = layer.eta_stdp.detach()\n",
    "\n",
    "    # Run training for the specified number of epochs\n",
    "    for epoch in range(self.epoch):\n",
    "        mod = 0  # Used to determine the learning rate annealment, resets at each epoch\n",
    "        # Run training for the specified number of timesteps\n",
    "        for spikes, labels in train_loader:\n",
    "            spikes, labels = spikes.to(self.device), labels.to(self.device)\n",
    "            idx = labels / self.filter # Set output index for spike forcing\n",
    "            # Pass through previous layers if they exist\n",
    "            if prev_layers:\n",
    "                with torch.no_grad():\n",
    "                    for prev_layer_name in prev_layers:\n",
    "                        prev_layer = getattr(self, prev_layer_name) # Get the previous layer object\n",
    "                        spikes = self.forward(spikes, prev_layer) # Pass spikes through the previous layer\n",
    "                        spikes = bn.clamp_spikes(spikes, prev_layer) # Clamp spikes [0, 0.9]\n",
    "            else:\n",
    "                prev_layer = None\n",
    "            # Get the output spikes from the current layer\n",
    "            pre_spike = spikes.detach() # Previous layer spikes for STDP\n",
    "            spikes = self.forward(spikes, layer) # Current layer spikes\n",
    "            spikes_noclp = spikes.detach() # Used for inhibitory homeostasis\n",
    "            spikes = bn.clamp_spikes(spikes, layer) # Clamp spikes [0, 0.9]\n",
    "            # Calculate STDP\n",
    "            layer = bn.calc_stdp(pre_spike,spikes,spikes_noclp,layer, idx, prev_layer=prev_layer)\n",
    "            # Adjust learning rates\n",
    "            layer = self._anneal_learning_rate(layer, mod, init_itp, init_stdp)\n",
    "            # Update the annealing mod & progress bar \n",
    "            mod += 1\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e068e",
   "metadata": {},
   "source": [
    "### 2.4 Create the forward pass\n",
    "\n",
    "Layers in VPRTempo are defined as an [nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html) layer, with incoming spikes being linearly transformed with the layer weights. The forward pass simply takes incoming spikes and caluclates the transform with positive and negative weights and adds them together, returning the transformed spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "68a22d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def forward(self, spikes, layer):\n",
    "    \"\"\"\n",
    "    Compute the forward pass of the model.\n",
    "\n",
    "    Parameters:\n",
    "    - spikes (Tensor): Input spikes.\n",
    "\n",
    "    Returns:\n",
    "    - Tensor: Output after processing.\n",
    "    \"\"\"\n",
    "\n",
    "    spikes = self.quant(spikes)\n",
    "    spikes = self.add.add(layer.exc(spikes), layer.inh(spikes))\n",
    "    spikes = self.dequant(spikes)\n",
    "\n",
    "    return spikes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb8e16d",
   "metadata": {},
   "source": [
    "### 2.5 Learning rate annealment & model loader/saver\n",
    "\n",
    "Finally, the last thing we will add to the model is the learning rate annealment regime and the functions for loading and saving trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e99f6d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def _anneal_learning_rate(self, layer, mod, itp, stdp):\n",
    "    \"\"\"\n",
    "    Anneal the learning rate for the current layer.\n",
    "    \"\"\"\n",
    "    if np.mod(mod, 100) == 0: # Modify learning rate every 100 timesteps\n",
    "        pt = pow(float(self.T - mod) / self.T, self.annl_pow)\n",
    "        layer.eta_ip = torch.mul(itp, pt) # Anneal intrinsic threshold plasticity learning rate\n",
    "        layer.eta_stdp = torch.mul(stdp, pt) # Anneal STDP learning rate\n",
    "\n",
    "    return layer\n",
    "\n",
    "def save_model(self, model_out):    \n",
    "    \"\"\"\n",
    "    Save the trained model to models output folder.\n",
    "    \"\"\"\n",
    "    torch.save(self.state_dict(), model_out) \n",
    "\n",
    "def load_model(self, model_path):\n",
    "    \"\"\"\n",
    "    Load pre-trained model and set the state dictionary keys.\n",
    "    \"\"\"\n",
    "    self.load_state_dict(torch.load(model_path, map_location=self.device),\n",
    "                         strict=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d65918",
   "metadata": {},
   "source": [
    "### 2.6 Initialize the model\n",
    "\n",
    "Now that the model has been defined, we can initialize it and start with the quantization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "55aa0e9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully added feature_layer\n",
      "Succesfully added output_layer\n",
      "VPRTempo succesfully initialized\n"
     ]
    }
   ],
   "source": [
    "model = VPRTempo()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88d4a18",
   "metadata": {},
   "source": [
    "### 2.7 Generate unique model name\n",
    "\n",
    "We will finally set up a unique model name based on the network architecture so we can save and reload our trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc786b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VPRTempo78415685001.pth\n"
     ]
    }
   ],
   "source": [
    "def generate_model_name(model):\n",
    "    \"\"\"\n",
    "    Generate the model name based on its parameters.\n",
    "    \"\"\"\n",
    "    return (\"VPRTempo\" +\n",
    "            str(model.input) +\n",
    "            str(model.feature) +\n",
    "            str(model.output) +\n",
    "            str(model.number_modules) +\n",
    "            '.pth')\n",
    "\n",
    "model_name = generate_model_name(model)\n",
    "\n",
    "print(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17640d20",
   "metadata": {},
   "source": [
    "## 3. Define the DataLoader\n",
    "\n",
    "### 3.1 Set the DataLoader\n",
    "\n",
    "Now that we've defined the model, we will set up the DataLoaders. These utilise a PyTorch CustomImageDataset and ProcessImage to import images and process them for training or inference. In brief, images are loaded, gamma corrected, resized, and then patch-normalized before being converted into system spikes to be propagated throughout.\n",
    "\n",
    "Since we present the network with one image at a time, the `batch_size` is kept to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6714bc35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import CustomImageDataset, ProcessImage\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "train_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                       img_dirs=model.training_dirs,\n",
    "                                       transform=image_transform,\n",
    "                                       skip=model.filter,\n",
    "                                       max_samples=model.number_training_images,\n",
    "                                       test=False)\n",
    "# Initialize the data loader\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=1, \n",
    "                          shuffle=False,\n",
    "                          num_workers=8,\n",
    "                          persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3067711",
   "metadata": {},
   "source": [
    "## 4. Quantization\n",
    "\n",
    "### 4.1 Model quantization\n",
    "\n",
    "VPRTempoQuant makes use of Quantized Aware Training QAT and has a few simple steps to prepare the model to accomodate this. First, we will get the default quantization configuration for `fggbem`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "36244802",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.quantization as quantization\n",
    "\n",
    "# Set the quantization configuration\n",
    "qconfig = quantization.get_default_qat_qconfig('fbgemm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a292cc",
   "metadata": {},
   "source": [
    "Next, we will set the model to be configured for network training and add our quantization configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a884ad96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the model to training mode and move to device\n",
    "model.train()\n",
    "model.to('cpu')\n",
    "model.qconfig = qconfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed34b802",
   "metadata": {},
   "source": [
    "Now we will convert the model over to QAT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "087f3b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply quantization configurations to the model\n",
    "model = quantization.prepare_qat(model, inplace=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f74ad2",
   "metadata": {},
   "source": [
    "At this point, we are ready to start training our network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3f4bdb",
   "metadata": {},
   "source": [
    "## 5. Set up and run the training \n",
    "\n",
    "### 5.1 Define and run the training regime\n",
    "\n",
    "The training will loop through each defined layer until every single one has trained. In order to propagate spikes throughout the system, trained layers are appended to a list so that they can be re-fed back into the network to calculate spikes based on learned weights.\n",
    "\n",
    "Run the below cell to train our `feature_layer` and `output_layer`!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0075638b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: feature_layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|████████████████████████████| 4000/4000 [01:30<00:00, 44.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training layer: output_layer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training : 100%|████████████████████████████| 4000/4000 [01:23<00:00, 48.00it/s]\n"
     ]
    }
   ],
   "source": [
    "# Keep track of trained layers to pass data through them\n",
    "trained_layers = [] \n",
    "\n",
    "# Training each layer\n",
    "for layer_name, _ in sorted(model.layer_dict.items(), key=lambda item: item[1]):\n",
    "    print(f\"Training layer: {layer_name}\")\n",
    "    # Retrieve the layer object\n",
    "    layer = getattr(model, layer_name)\n",
    "    # Train the layer\n",
    "    model.train_model(train_loader, layer, prev_layers=trained_layers)\n",
    "    # After training the current layer, add it to the list of trained layers\n",
    "    trained_layers.append(layer_name)\n",
    "    \n",
    "print('All layers trained succesfully')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6daea3",
   "metadata": {},
   "source": [
    "### 5.2 Convert and save the model\n",
    "\n",
    "Now that the training has been completed, we can convert the QAT model over to be fully quantized. As the layers were trained, scale and zero-point factors will learned for all the elements of the model and can now be applied to the layers. Once converted, we will save the model for use in inferencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "53ededaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Convert the model to a quantized model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model \u001b[38;5;241m=\u001b[39m quantization\u001b[38;5;241m.\u001b[39mconvert(model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Save the model\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:551\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    550\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 551\u001b[0m _convert(\n\u001b[1;32m    552\u001b[0m     module, mapping, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_reference\u001b[38;5;241m=\u001b[39mis_reference,\n\u001b[1;32m    553\u001b[0m     convert_custom_config_dict\u001b[38;5;241m=\u001b[39mconvert_custom_config_dict)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    555\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:589\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 589\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    590\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m    591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    589\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    590\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    594\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:624\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    622\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n\u001b[1;32m    625\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:277\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeight observer must have dtype torch.qint8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    276\u001b[0m qweight \u001b[38;5;241m=\u001b[39m _quantize_weight(mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat(), weight_post_process)\n\u001b[0;32m--> 277\u001b[0m qlinear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(mod\u001b[38;5;241m.\u001b[39min_features,\n\u001b[1;32m    278\u001b[0m               mod\u001b[38;5;241m.\u001b[39mout_features,\n\u001b[1;32m    279\u001b[0m               dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    280\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, mod\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    281\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(act_scale)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:151\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias_, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported dtype specified for quantized Linear!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m LinearPackedParams(dtype)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, bias)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:27\u001b[0m, in \u001b[0;36mLinearPackedParams.__init__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     26\u001b[0m     wq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight_bias(wq, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:32\u001b[0m, in \u001b[0;36mLinearPackedParams.set_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch\u001b[38;5;241m.\u001b[39mTensor, bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack(weight, bias)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack_fp16(weight, bias)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "# Convert the model to a quantized model\n",
    "model = quantization.convert(model, inplace=False)\n",
    "model.eval()\n",
    "# Save the model\n",
    "model.save_model(os.path.join('./models', model_name))  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d69843",
   "metadata": {},
   "source": [
    "## 6. Inferencing\n",
    "\n",
    "As in the previous tutorial, inferencing with a trained model is quite simple. The only additional thing we need to do is reinitialize the VPRTempo class and convert it to quantized before loading the model. Without pre-quantizing the inference model, state dictionary keys will not match since all the layers and associated components have new parameters such as scale and zero-point.\n",
    "\n",
    "### 6.1 Add the inference function to the VPRTempo class\n",
    "\n",
    "We will start by adding in the inference function to VPRTempo. It is similar to the training regime but omits the learning components `calc_stdp` and simply runs through all the layers until it reaches the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a368c532",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%add_to VPRTempo\n",
    "def evaluate(self, test_loader, layers=None):\n",
    "    \"\"\"\n",
    "    Run the inferencing model and calculate the accuracy.\n",
    "\n",
    "    :param test_loader: Testing data loader\n",
    "    :param layers: Layers to pass data through\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize the number of correct predictions\n",
    "    numcorr = 0\n",
    "    idx = 0\n",
    "\n",
    "    # Initialize the tqdm progress bar\n",
    "    pbar = tqdm(total=self.number_testing_images,\n",
    "                desc=\"Running the test network\",\n",
    "                position=0)\n",
    "\n",
    "    # Run inference for the specified number of timesteps\n",
    "    for spikes, labels in test_loader:\n",
    "        # Set device\n",
    "        spikes, labels = spikes.to(self.device), labels.to(self.device)\n",
    "        # Pass through previous layers if they exist\n",
    "        if layers:\n",
    "            for layer_name in layers:\n",
    "                layer = getattr(self, layer_name)\n",
    "                spikes = self.forward(spikes, layer)\n",
    "                spikes = bn.clamp_spikes(spikes, layer)\n",
    "\n",
    "        # Evaluate if the prediction is correct\n",
    "        if torch.argmax(spikes.reshape(1, self.number_training_images)) == idx:\n",
    "            numcorr += 1\n",
    "\n",
    "        # Update the index and progress bar\n",
    "        idx += 1\n",
    "        pbar.update(1)\n",
    "\n",
    "    # Close the tqdm progress bar\n",
    "    pbar.close()\n",
    "    # Calculate and record the accuracy\n",
    "    accuracy = round((numcorr/self.number_testing_images)*100,2)\n",
    "    model.logger.info(\"P@100R: \"+ str(accuracy) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e841fc7",
   "metadata": {},
   "source": [
    "### 6.2 Define the inferencing DataLoader\n",
    "\n",
    "The only difference between the training and testing DataLoader is the directory with which it will import images from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4a1adc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the image transforms and datasets\n",
    "image_transform = ProcessImage(model.dims, model.patches)\n",
    "test_dataset = CustomImageDataset(annotations_file=model.dataset_file, \n",
    "                                  img_dirs=model.testing_dirs,\n",
    "                                  transform=image_transform,\n",
    "                                  skip=model.filter,\n",
    "                                  max_samples=model.number_testing_images)\n",
    "# Initialize the data loader\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=1, \n",
    "                         shuffle=False,\n",
    "                         num_workers=8,\n",
    "                         persistent_workers=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "018de09a",
   "metadata": {},
   "source": [
    "### 6.3 Re-initialize the model class, convert to quantization, and load the model\n",
    "\n",
    "Now we will re-initialize the VPRTempo class model, set to eval mode, and convert it over to quantized so that we can import our newly trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "30d51e96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succesfully added feature_layer\n",
      "Succesfully added output_layer\n",
      "VPRTempo succesfully initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adam/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/observer.py:214: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/Users/adam/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/utils.py:310: UserWarning: must run observer before calling calculate_qparams. Returning default values.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Didn't find engine for operation quantized::linear_prepack NoQEngine",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# Prepare and convert the model to a quantized model\u001b[39;00m\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m quantization\u001b[38;5;241m.\u001b[39mprepare(model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m quantization\u001b[38;5;241m.\u001b[39mconvert(model, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Load the model\u001b[39;00m\n\u001b[1;32m     13\u001b[0m model\u001b[38;5;241m.\u001b[39mload_model(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./models\u001b[39m\u001b[38;5;124m'\u001b[39m, model_name))\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:551\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(module, mapping, inplace, remove_qconfig, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m inplace:\n\u001b[1;32m    550\u001b[0m     module \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(module)\n\u001b[0;32m--> 551\u001b[0m _convert(\n\u001b[1;32m    552\u001b[0m     module, mapping, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, is_reference\u001b[38;5;241m=\u001b[39mis_reference,\n\u001b[1;32m    553\u001b[0m     convert_custom_config_dict\u001b[38;5;241m=\u001b[39mconvert_custom_config_dict)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_qconfig:\n\u001b[1;32m    555\u001b[0m     _remove_qconfig(module)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:589\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, mod \u001b[38;5;129;01min\u001b[39;00m module\u001b[38;5;241m.\u001b[39mnamed_children():\n\u001b[1;32m    585\u001b[0m     \u001b[38;5;66;03m# both fused modules and observed custom modules are\u001b[39;00m\n\u001b[1;32m    586\u001b[0m     \u001b[38;5;66;03m# swapped as one unit\u001b[39;00m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[0;32m--> 589\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    590\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[1;32m    591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:591\u001b[0m, in \u001b[0;36m_convert\u001b[0;34m(module, mapping, inplace, is_reference, convert_custom_config_dict)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mod, _FusedModule) \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    588\u001b[0m        type_before_parametrizations(mod) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m custom_module_class_mapping:\n\u001b[1;32m    589\u001b[0m         _convert(mod, mapping, \u001b[38;5;28;01mTrue\u001b[39;00m,  \u001b[38;5;66;03m# inplace\u001b[39;00m\n\u001b[1;32m    590\u001b[0m                  is_reference, convert_custom_config_dict)\n\u001b[0;32m--> 591\u001b[0m     reassign[name] \u001b[38;5;241m=\u001b[39m swap_module(mod, mapping, custom_module_class_mapping)\n\u001b[1;32m    593\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m reassign\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    594\u001b[0m     module\u001b[38;5;241m.\u001b[39m_modules[key] \u001b[38;5;241m=\u001b[39m value\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/quantization/quantize.py:624\u001b[0m, in \u001b[0;36mswap_module\u001b[0;34m(mod, mapping, custom_module_class_mapping)\u001b[0m\n\u001b[1;32m    622\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod, weight_qparams)\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 624\u001b[0m         new_mod \u001b[38;5;241m=\u001b[39m qmod\u001b[38;5;241m.\u001b[39mfrom_float(mod)\n\u001b[1;32m    625\u001b[0m     swapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m swapped:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Preserve module's pre forward hooks. They'll be called on quantized input\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:277\u001b[0m, in \u001b[0;36mLinear.from_float\u001b[0;34m(cls, mod)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mWeight observer must have dtype torch.qint8\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    276\u001b[0m qweight \u001b[38;5;241m=\u001b[39m _quantize_weight(mod\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mfloat(), weight_post_process)\n\u001b[0;32m--> 277\u001b[0m qlinear \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(mod\u001b[38;5;241m.\u001b[39min_features,\n\u001b[1;32m    278\u001b[0m               mod\u001b[38;5;241m.\u001b[39mout_features,\n\u001b[1;32m    279\u001b[0m               dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m    280\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, mod\u001b[38;5;241m.\u001b[39mbias)\n\u001b[1;32m    281\u001b[0m qlinear\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(act_scale)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:151\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias_, dtype)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported dtype specified for quantized Linear!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m LinearPackedParams(dtype)\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params\u001b[38;5;241m.\u001b[39mset_weight_bias(qweight, bias)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscale \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:27\u001b[0m, in \u001b[0;36mLinearPackedParams.__init__\u001b[0;34m(self, dtype)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     26\u001b[0m     wq \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros([\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_weight_bias(wq, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/ao/nn/quantized/modules/linear.py:32\u001b[0m, in \u001b[0;36mLinearPackedParams.set_weight_bias\u001b[0;34m(self, weight, bias)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mexport\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mset_weight_bias\u001b[39m(\u001b[38;5;28mself\u001b[39m, weight: torch\u001b[38;5;241m.\u001b[39mTensor, bias: Optional[torch\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mqint8:\n\u001b[0;32m---> 32\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack(weight, bias)\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mfloat16:\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_packed_params \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mops\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mlinear_prepack_fp16(weight, bias)\n",
      "File \u001b[0;32m~/mambaforge/envs/vprtempo/lib/python3.11/site-packages/torch/_ops.py:502\u001b[0m, in \u001b[0;36mOpOverloadPacket.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    497\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    498\u001b[0m     \u001b[38;5;66;03m# overloading __call__ to ensure torch.ops.foo.bar()\u001b[39;00m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;66;03m# is still callable from JIT\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# We save the function ptr as the `op` attribute on\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     \u001b[38;5;66;03m# OpOverloadPacket to access it here.\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_op(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs \u001b[38;5;129;01mor\u001b[39;00m {})\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Didn't find engine for operation quantized::linear_prepack NoQEngine"
     ]
    }
   ],
   "source": [
    "# Set the model to evaluation mode and set configuration\n",
    "model = VPRTempo()\n",
    "model.eval()\n",
    "model.qconfig = qconfig\n",
    "\n",
    "# Apply quantization configurations to all layers in layer_dict\n",
    "for layer_name, _ in model.layer_dict.items():\n",
    "    getattr(model, layer_name).qconfig = qconfig\n",
    "# Prepare and convert the model to a quantized model\n",
    "model = quantization.prepare(model, inplace=False)\n",
    "model = quantization.convert(model, inplace=False)\n",
    "# Load the model\n",
    "model.load_model(os.path.join('./models', model_name))\n",
    "\n",
    "# Retrieve layer names for inference\n",
    "layer_names = list(model.layer_dict.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472d24e8",
   "metadata": {},
   "source": [
    "### 6.4 Run the model inference\n",
    "\n",
    "Now we are ready to inference the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37aa84e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Use evaluate method for inference accuracy\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model\u001b[38;5;241m.\u001b[39mevaluate(test_loader, layers\u001b[38;5;241m=\u001b[39mlayer_names)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_names' is not defined"
     ]
    }
   ],
   "source": [
    "# Use evaluate method for inference accuracy\n",
    "model.evaluate(test_loader, layers=layer_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f46e43",
   "metadata": {},
   "source": [
    "## 7. Conslusions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763e86c5",
   "metadata": {},
   "source": [
    "This tutorial covered how we can convert the VPRTempo model to perform Quantized Aware Training (QAT) to keep the model size more lightweight. You might notice that if you compare the system between FP32 to Int8, the model works equally as well with a reduced bit-depth with the added benefit of a reduced model size.\n",
    "\n",
    "To read more about QAT and quantization in general, PyTorch provides many useful articles;\n",
    "https://pytorch.org/docs/stable/quantization.html\n",
    "https://pytorch.org/blog/quantization-in-practice/\n",
    "\n",
    "The key benefit to this is being able to perform fast training and inferencing on CPU architecture, which for resource limited compute scenarios is critical."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
