{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34c7b8a-e7bb-47f4-b558-be1bde9a7b37",
   "metadata": {},
   "source": [
    "## VPRTempo & VPRTempoQuant - Basic Demo\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This is a basic, extremely simplified version of VPRTempo that highlights how images are transformed, spikes and weights are used, and the readout for performance using a model trained using our base system and the Quantized Aware Training (QAT) version. This is a basic, extremely simplified version of VPRTempo that highlights how images are transformed, spikes and weights are used, and the readout for performance. Although the proper implementation is in [PyTorch](https://pytorch.org/), we present a simple NumPy example to get started. As in the paper, we will present a simple example using the [Nordland](https://webdiis.unizar.es/~jmfacil/pr-nordland/#download-dataset) dataset with pre-trained set of weights.\n",
    "\n",
    "Before starting, make sure the following packages are installed and imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879cd02-82db-441d-9476-fff1925bf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprt opencv-python, NumPy, and matplotlib.pyplot\n",
    "try:\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    !pip3 install numpy, opencv-python, matplotlib # pip install if modules not present\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b11853-6e17-4884-ac92-d35d814add42",
   "metadata": {},
   "source": [
    "Next, we will need to get the pretrained weights for the model. To get them and the other materials for the , we will use [Git Large File Storage](https://git-lfs.com/) to download the required materials. Please ensure you have it downloaded on your local machine prior to running these tutorials."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be03b081",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# Run Git LFS pull command\n",
    "try:\n",
    "    result = subprocess.run(\"git lfs pull\", shell=True, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE, universal_newlines=True)\n",
    "    print(\"Git LFS pull successful.\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"Git LFS pull failed with error:\\n{e.stderr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45df38-e333-46b2-9161-80e6ac367532",
   "metadata": {},
   "source": [
    "### Image processing\n",
    "\n",
    "Let's have a look at how we process our images to run through VPRTempo. We utilize a technique called *patch normalization* to resize input images and normalize the pixel intensities. To start, let's see what the original image looks like before patch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f129b5-9a7a-4b50-9d94-b9bf512f8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image\n",
    "raw_img = cv2.imread('./mats/1_BasicDemo/summer.png')\n",
    "rgb_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('Nordland Summer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cf25e-35ae-4885-9cf1-c1b09ce4ad42",
   "metadata": {},
   "source": [
    "What we have here is a 360x640 RGB image, which for processing through neural networks is too big (230,400 total pixels). So instead, we'll use patch normalization to reduce the image size down to a grayscale 56x56 image to just 3136 pixels in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67656a-3ba4-4374-b780-4e8bac4ec2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the patch normalized image\n",
    "patch_img = np.load('./mats/1_BasicDemo/summer_patchnorm.npy', allow_pickle=True)\n",
    "\n",
    "# Plot the image\n",
    "plt.matshow(patch_img)\n",
    "plt.title('Nordland Summer Patch Normalized')\n",
    "plt.colorbar(shrink=0.75, label=\"Pixel intensity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404dfd2-10bd-4981-96ef-6092a9866fc6",
   "metadata": {},
   "source": [
    "The reduced image dimensions with patch normalization allows for a decent representation of the full scene, despite the smaller size.\n",
    "\n",
    "### Convert images to spikes\n",
    "\n",
    "'Spikes' in the context of VPRTempo are a little different than conventional spiking neural networks. Typically, spikes from image datasets are converted into Poisson spike trains where the pixel intensity determines the number of spikes to propagate throughout a network. VPRTempo only considers each pixel as a single spike, but considers the *amplitude* of the spike to determine the timing within a single timestep - where large amplitudes (high pixel intensity) spike early in a timestep, and vice versa for small amplitudes. \n",
    "\n",
    "Let's flatten the patch normalized image into a 1D-array so we can apply our network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6ae95-2a79-4b45-8a60-503079339739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 2D image to a 1D-array\n",
    "patch_1d = np.reshape(patch_img, (3136,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a5eaf-1de3-461f-b138-3ac820da8bae",
   "metadata": {},
   "source": [
    "### Load the pre-trained network weights\n",
    "\n",
    "Our network consists of the following architecture:\n",
    "\n",
    "    - An input layer sparsely connected to a feature layer, 3136 input neurons to 6272 feature neurons\n",
    "    - The feature layer fully connected to a one-hot-encoded output layer, 6272 feature neurons to 500 output neurons\n",
    "\n",
    "Each layer connection is trained separately and stored in different weight matrices for excitatory (positive) and inhibitory (negative) connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98749a-8f28-477b-871c-93626e96786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input to feature excitatory and inhibitory network weights\n",
    "featureW = np.load('./mats/1_BasicDemo/featureW.npy')\n",
    "\n",
    "# Plot the  weights\n",
    "plt.matshow(featureW.T)\n",
    "plt.title('Input > Feature Weights')\n",
    "plt.colorbar(shrink=0.8, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826213d7-7721-440c-b1a8-47fb613339eb",
   "metadata": {},
   "source": [
    "Whilst it might be a little difficult to see, our excitatory connection amplitudes are on average a little higher than our inhibitiory. However, we overall have more inhibitiory connections that positive to balance the system.\n",
    "\n",
    "This is because when we set up our connections we use a probability of connections for both excitation and inbhition. In this case, we have a 10% connection probability for excitatory weights and a 50% probability for inhibitiory. This means as well there will be a high number of neurons without connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94acb2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this function, we will plot and visualize the distribution of weights and connections.\n",
    "def count_and_plot(array):\n",
    "    # Flatten the 2D array and count positive, negative, and zero values\n",
    "    flattened_array = array.flatten()\n",
    "    positive_count = np.sum(flattened_array > 0)\n",
    "    negative_count = np.sum(flattened_array < 0)\n",
    "    zero_count = np.sum(flattened_array == 0)\n",
    "    \n",
    "    # Calculate percentages\n",
    "    total_count = flattened_array.size\n",
    "    positive_percentage = (positive_count / total_count) * 100\n",
    "    negative_percentage = (negative_count / total_count) * 100\n",
    "    zero_percentage = (zero_count / total_count) * 100\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Excitatory Connections: {positive_count} ({positive_percentage:.2f}%)\")\n",
    "    print(f\"Inhibitory Conncetions: {negative_count} ({negative_percentage:.2f}%)\")\n",
    "    print(f\"Zero Connections: {zero_count} ({zero_percentage:.2f}%)\")\n",
    "\n",
    "    # Create a bar plot of the percentages\n",
    "    categories = ['Excitatory', 'Inhibitory', 'Zero']\n",
    "    percentages = [positive_percentage, negative_percentage, zero_percentage]\n",
    "\n",
    "    plt.bar(categories, percentages)\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Percentage')\n",
    "    plt.title('Percentage of Excitatory, Inhibitiory, and Zero Connections')\n",
    "    plt.ylim(0, 60)  # Set the y-axis limit to 0-60%\n",
    "    plt.show()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Call the function to count and plot\n",
    "    count_and_plot(featureW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f180220e",
   "metadata": {},
   "source": [
    "Now let's have a look at the feature to the output weights, and see how the distribution of excitiatory and inhibitory connections differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609eae5-f584-4c98-9eb6-3c3cf1e2aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input to feature excitatory and inhibitory network weights\n",
    "outputW = np.load('./mats/1_BasicDemo/outputW.npy')\n",
    "\n",
    "# Plot the  weights\n",
    "plt.matshow(outputW)\n",
    "plt.title('Feature > Output Weights')\n",
    "plt.colorbar(shrink=0.8, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n",
    "# Plot the distributions\n",
    "count_and_plot(outputW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591969a-e72e-43b2-8c89-16a13bb29fe6",
   "metadata": {},
   "source": [
    "### Propagate network spikes\n",
    "\n",
    "Now we'll propagate the input spikes across the layers to get the output. All we have to do is multiply the input spikes by the Input > Feature weights for both excitatory and inhibitory matrices and add them, then take the feature spikes and multiply them by the Feature > Output weights and do the smae thing. We'll also clamp spikes in the range of [0, 0.9] to prevent negative spikes and spike explosions.\n",
    "\n",
    "Let's do that and visualize the spikes as they're going through, we'll start with the Input to Feature layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c84239-c176-48c3-8954-25da5f989d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature spikes (positive and negative weights)\n",
    "feature_spikes = np.matmul(featureW,patch_1d)\n",
    "feature_spikes = np.clip(feature_spikes, 0, 0.9)\n",
    "\n",
    "# Now create the line plot\n",
    "plt.plot(np.arange(len(feature_spikes)), feature_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Feature Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0b0a3-66fc-4202-963c-cbd05114d283",
   "metadata": {},
   "source": [
    "This looks a little homogenous, but this is the feature representation of our input image. \n",
    "\n",
    "Now let's propagate the feature layer spikes through to the output layer to get our corresponding place match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4dc99-c7b9-4e9b-ba7c-58f6e30631cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate output spikes (positive and negative weights)\n",
    "output_spikes = np.matmul(outputW,feature_spikes)\n",
    "output_spikes = np.clip(output_spikes, 0, 0.9)\n",
    "\n",
    "# Now create the line plot\n",
    "plt.plot(np.arange(len(output_spikes)), output_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Output Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4f5f6-017b-4d7d-812a-c96baf9cb39f",
   "metadata": {},
   "source": [
    "Success! We have propagated our input spikes across the layers to reach this output. Clearly, one of the output spikes has the highest amplitude. Our network weights were trained on 500 locations from a Fall and Spring traversal of Nordland. For this example, we passed the first location from the Summer traversal through the network to achieve this output - which clearly looks to have spikes Neuron ID '0' the highest!\n",
    "\n",
    "Let's prove that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780371ca-9dfe-4dd7-857d-e35be73ffd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the argmax from the output spikes\n",
    "prediction = np.argmax(output_spikes)\n",
    "print(f\"Neuron ID with the highest output is {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c82d7",
   "metadata": {},
   "source": [
    "## Quantized model example\n",
    "\n",
    "Now that we have seen how our base model works, let's look at how our int8 quantized model performs by comparison. Working in the in8 space has a few benefits, like faster inferencing time and smaller model sizes. There are a couple differences however when feeding spikes throughout the system that PyTorch performs in the backend.\n",
    "\n",
    "Let's start by converting our input image into int8 spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c893e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting fp32 spikes to int8 uses a learned scale factor during quantization aware training\n",
    "spike_scale = 133\n",
    "patch_img_int = patch_img*spike_scale\n",
    "\n",
    "# Plot the converted int8 image\n",
    "plt.matshow(patch_img_int)\n",
    "plt.title('Nordland Summer Patch Normalized Int8')\n",
    "plt.colorbar(shrink=0.75, label=\"Pixel intensity\")\n",
    "plt.show()\n",
    "\n",
    "# Convert 2D image to a 1D-array\n",
    "patch_1d_int = np.reshape(patch_img_int, (3136,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f68d3dc",
   "metadata": {},
   "source": [
    "Now we'll load in and plot our integer based weights, as well as some scale factors which will be important to reduce the size of our spikes after multiplying them with our weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c99f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the scales for the feature and output spikes\n",
    "feature_scales = np.load('./mats/1_BasicDemo/featureScales.npy',allow_pickle=True)\n",
    "output_scales = np.load('./mats/1_BasicDemo/outputScales.npy',allow_pickle=True)\n",
    "\n",
    "# Load the int8 weights and plot them\n",
    "featureQuantW = np.load('./mats/1_BasicDemo/featureQuantW.npy')\n",
    "outputQuantW = np.load('./mats/1_BasicDemo/outputQuantW.npy')\n",
    "\n",
    "# Plot the feature weights\n",
    "plt.matshow(featureQuantW.T)\n",
    "plt.title('Input > Feature Weights')\n",
    "plt.colorbar(shrink=0.8, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n",
    "# Plot the output weights\n",
    "plt.matshow(outputQuantW)\n",
    "plt.title('Feature > Output Weights')\n",
    "plt.colorbar(shrink=0.8, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f185e3ff",
   "metadata": {},
   "source": [
    "Now as above, let's propagate the input spikes throughout the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb1ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the feature spikes\n",
    "feature_spikes_int = np.matmul(featureQuantW,patch_1d_int)\n",
    "\n",
    "# Now create the line plot\n",
    "plt.plot(np.arange(len(feature_spikes_int)), feature_spikes_int)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Output Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9189aa",
   "metadata": {},
   "source": [
    "Those are some big spikes! We're going to have to scale these spikes back down before we forward them to the output layer, otherwise we'll have some huge activations. Let's take those scales we loaded in earlier and apply them to the feature spikes.\n",
    "\n",
    "We have three things to consider here:\n",
    " - A slice scale factor (per neuronal connection scale)\n",
    " - A zero point (a factor to change where 'zero' is)\n",
    " \n",
    " Let's print out these three factors and see how they scale our spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95032ac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the individual scales\n",
    "print(f\"The slice scale factor is {feature_scales[1]}\")\n",
    "print(f\"The zero point is {feature_scales[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f62b909",
   "metadata": {},
   "source": [
    "Now we'll modify and scale our spikes to then pass them on to the feature layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7dd83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the feature spikes\n",
    "scaled_feature_spikes = (feature_spikes_int//(feature_scales[1]))+feature_scales[2]\n",
    "scaled_feature_spikes = np.clip(scaled_feature_spikes,0,255)\n",
    "\n",
    "# Plot the scaled feature spikes\n",
    "plt.plot(np.arange(len(scaled_feature_spikes)), scaled_feature_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Output Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a767c92",
   "metadata": {},
   "source": [
    "Now that we've scaled our feature spikes, let's pass them through to the output layer and get our match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7519c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the output spikes\n",
    "output_spikes_int = np.matmul(outputQuantW,scaled_feature_spikes)\n",
    "\n",
    "# Scale the output spikes\n",
    "scaled_output_spikes = output_spikes_int//(output_scales[1]) + output_scales[2]\n",
    "\n",
    "# Plot the scaled feature spikes\n",
    "plt.plot(np.arange(len(scaled_output_spikes)), scaled_output_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Output Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc4588a",
   "metadata": {},
   "source": [
    "And once again, as in the base model, we can see that output neuron 0 is the highest respondant.\n",
    "\n",
    "Let's prove it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e457e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the argmax from the output spikes\n",
    "prediction = np.argmax(scaled_output_spikes)\n",
    "print(f\"Neuron ID with the highest output is {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8a7fb-66b4-455b-922e-b0fdc38b53c5",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "We have gone through a very basic demo of how VPRTempo takes input images, patch normalizes them, and propagates the spikes throughout the weights to achieve the desired matching output. Although this demonstration was performed using NumPy, the torch implementation is virtually the same except we use tensors with or without quantization. \n",
    "\n",
    "We also went through how the quantization version of the network handled weights and spikes in the integer domain.\n",
    "\n",
    "If you would like to go more in-depth with training and inferencing, checkout some of the [other tutorials](https://github.com/AdamDHines/VPRTempo-quant/tree/main/tutorials) which show you how to train your own model and goes through the more sophisticated implementation of VPRTempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
