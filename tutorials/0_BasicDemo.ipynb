{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34c7b8a-e7bb-47f4-b558-be1bde9a7b37",
   "metadata": {},
   "source": [
    "## VPRTempo - Basic Demo\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This is a basic, extremely simplified version of VPRTempo that highlights how images are transformed, spikes and weights are used, and the readout for performance. Although the proper implementation is in [PyTorch](https://pytorch.org/), we present a simple NumPy example to get started. As in the paper, we will present a simple example using the [Nordland](https://webdiis.unizar.es/~jmfacil/pr-nordland/#download-dataset) dataset with a pre-trained model.\n",
    "\n",
    "Before starting, make sure the following packages are installed and imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879cd02-82db-441d-9476-fff1925bf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprt opencv-python, NumPy, and matplotlib.pyplot\n",
    "try:\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    ! pip install numpy, opencv-python, matplotlib # pip install if modules not present\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45df38-e333-46b2-9161-80e6ac367532",
   "metadata": {},
   "source": [
    "### Image processing\n",
    "\n",
    "Let's have a look at how we process our images to run through VPRTempo. We utilize a technique called *patch normalization* to resize input images and normalize the pixel intensities. To start, let's see what the original image looks like before patch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f129b5-9a7a-4b50-9d94-b9bf512f8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image\n",
    "raw_img = cv2.imread('./mats/0_basicdemo/summer.png')\n",
    "rgb_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "\n",
    "# Plot the image\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('Nordland Summer')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cf25e-35ae-4885-9cf1-c1b09ce4ad42",
   "metadata": {},
   "source": [
    "What we have here is a 360x640 RGB image, which for processing through neural networks is too big (230,400 total pixels). So instead, we'll use patch normalization to reduce the image size down to a grayscale 28x28 image to just 784 pixels in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67656a-3ba4-4374-b780-4e8bac4ec2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the patch normalized image\n",
    "patch_img = np.load('./mats/0_basicdemo/summer_patchnorm.npy', allow_pickle=True)\n",
    "\n",
    "# Plot the image\n",
    "plt.matshow(patch_img)\n",
    "plt.title('Nordland Summer Patch Normalized')\n",
    "plt.colorbar(shrink=0.75, label=\"Pixel intensity\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d404dfd2-10bd-4981-96ef-6092a9866fc6",
   "metadata": {},
   "source": [
    "The reduced image dimensions with patch normalization allows for a decent representation of the full scene, despite the smaller size.\n",
    "\n",
    "### Convert images to spikes\n",
    "\n",
    "'Spikes' in the context of VPRTempo are a little different than conventional spiking neural networks. Typically, spikes from image datasets are converted into Poisson spike trains where the pixel intensity determines the number of spikes to propagate throughout a network. VPRTempo only considers each pixel as a single spike, but considers the *amplitude* of the spike to determine the timing within a single timestep - where large amplitudes (high pixel intensity) spike early in a timestep, and vice versa for small amplitudes. \n",
    "\n",
    "Let's flatten the patch normalized image into a 1D-array so we can apply our network weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6ae95-2a79-4b45-8a60-503079339739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 2D image to a 1D-array\n",
    "patch_1d = np.reshape(patch_img, (784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a5eaf-1de3-461f-b138-3ac820da8bae",
   "metadata": {},
   "source": [
    "### Load the pre-trained network weights\n",
    "\n",
    "Our network consists of the following architecture:\n",
    "\n",
    "    - An input layer sparsely connected to a feature layer, 784 input neurons to 1568 feature neurons\n",
    "    - The feature layer fully connected to a one-hot-encoded output layer, 1568 feature neurons to 500 output neurons\n",
    "\n",
    "Each layer connection is trained separately and stored in different weight matrices for excitatory (positive) and inhibitory (negative) connections. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98749a-8f28-477b-871c-93626e96786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input to feature excitatory and inhibitory network weights\n",
    "if_exc = np.load('./mats/0_basicdemo/if_exc.npy')\n",
    "if_inh = np.load('./mats/0_basicdemo/if_inh.npy')\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) # Adjust the figure size as needed\n",
    "\n",
    "# Plot the excitatory weights\n",
    "exc_plot = axes[0].matshow(if_exc.T)\n",
    "axes[0].set_title('Input > Feature Excitatory Weights')\n",
    "fig.colorbar(exc_plot, ax=axes[0], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Plot the inhibitory weights\n",
    "inh_plot = axes[1].matshow(if_inh.T, cmap='viridis_r')\n",
    "axes[1].set_title('Input > Feature Inhibitory Weights')\n",
    "fig.colorbar(inh_plot, ax=axes[1], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "826213d7-7721-440c-b1a8-47fb613339eb",
   "metadata": {},
   "source": [
    "In this case, we have more inhibitory connections than we do excitatory for the input to feature layer. Let's load the feature to output layer spikes and visualise them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7609eae5-f584-4c98-9eb6-3c3cf1e2aa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input to feature excitatory and inhibitory network weights\n",
    "fo_exc = np.load('./mats/0_basicdemo/fo_exc.npy')\n",
    "fo_inh = np.load('./mats/0_basicdemo/fo_inh.npy')\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) # Adjust the figure size as needed\n",
    "\n",
    "# Plot the excitatory weights\n",
    "exc_plot = axes[0].matshow(fo_exc)\n",
    "axes[0].set_title('Feature > Output Excitatory Weights')\n",
    "fig.colorbar(exc_plot, ax=axes[0], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Plot the inhibitory weights\n",
    "inh_plot = axes[1].matshow(fo_inh, cmap='viridis_r')\n",
    "axes[1].set_title('Feature > Output Inhibitory Weights')\n",
    "fig.colorbar(inh_plot, ax=axes[1], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591969a-e72e-43b2-8c89-16a13bb29fe6",
   "metadata": {},
   "source": [
    "### Propagate network spikes\n",
    "\n",
    "Now we'll propagate the input spikes across the layers to get the output. All we have to do is multiply the input spikes by the Input > Feature weights for both excitatory and inhibitory matrices and add them, then take the feature spikes and multiply them by the Feature > Output weights and do the smae thing. We'll also clamp spikes in the range of [0, 0.9] to prevent negative spikes and spike explosions.\n",
    "\n",
    "Let's do that and visualize the spikes as they're going through, we'll start with the Input to Feature layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c84239-c176-48c3-8954-25da5f989d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature spikes (positive and negative weights)\n",
    "feature_spikes = np.matmul(if_exc,patch_1d) + np.matmul(if_inh,patch_1d)\n",
    "feature_spikes = np.clip(feature_spikes, 0, 0.9)\n",
    "\n",
    "# Now create the line plot\n",
    "plt.plot(np.arange(len(feature_spikes)), feature_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Feature Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0b0a3-66fc-4202-963c-cbd05114d283",
   "metadata": {},
   "source": [
    "Now let's propagate the feature layer spikes through to the output layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4dc99-c7b9-4e9b-ba7c-58f6e30631cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate output spikes (positive and negative weights)\n",
    "output_spikes = np.matmul(fo_exc,feature_spikes) + np.matmul(fo_inh,feature_spikes)\n",
    "output_spikes = np.clip(output_spikes, 0, 0.9)\n",
    "\n",
    "# Now create the line plot\n",
    "plt.plot(np.arange(len(output_spikes)), output_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Output Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4f5f6-017b-4d7d-812a-c96baf9cb39f",
   "metadata": {},
   "source": [
    "Success! We have propagated our input spikes across the layers to reach this output. Clearly, one of the output spikes has the highest amplitude. Our network weights were trained on 500 locations from a Fall and Spring traversal of Nordland. For this example, we passed the first location from the Summer traversal through the network to achieve this output - which clearly looks to have spikes Neuron ID '0' the highest!\n",
    "\n",
    "Let's prove that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780371ca-9dfe-4dd7-857d-e35be73ffd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the argmax from the output spikes\n",
    "prediction = np.argmax(output_spikes)\n",
    "print(f\"Neuron ID with the highest output is {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8a7fb-66b4-455b-922e-b0fdc38b53c5",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "We have gone through a very basic demo of how VPRTempo takes input images, patch normalizes them, and propagates the spikes throughout the weights to achieve the desired matching output. Although this demonstration was performed using NumPy, the torch implementation is virtually the same except we use tensors with or without quantization. \n",
    "\n",
    "The purpose of splitting up excitatory and inhibitory weights is to allow for extra hometostatic normalization of inhibitory connections, which has proven to be critical in regulating overall system activity.\n",
    "\n",
    "If you would like to go more in-depth with training and inferencing, checkout some of the [other tutorials](https://github.com/AdamDHines/VPRTempo-quant/tree/main/tutorials) which show you how to train your own model and goes through the more sophisticated implementation of VPRTempo."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
