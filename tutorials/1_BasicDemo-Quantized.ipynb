{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b34c7b8a-e7bb-47f4-b558-be1bde9a7b37",
   "metadata": {},
   "source": [
    "## VPRTempoQuant - Basic Demo for a quantized version of VPRTempo\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "This is a basic, extremely simplified version of VPRTempo that highlights how images are transformed, spikes and weights are used, and the readout for performance using a model trained using Quantized Aware Training (QAT). We will view the system through the lens of integer based weights and spikes to see how a quantized version of VPRTempo operates under the hood.\n",
    "\n",
    "*Note: In this example, we will lose some amount of precision because we are only using integers for all calculations. In the deployed version, PyTorch quantizes and dequantizes spikes and weights so that some calculations are performed in the floating point domain. As such, this tutorial should be taken purely for conceptual understanding of a quantized version of VPRTempo and not for implementation purposes.*\n",
    "\n",
    "Before starting, make sure the following packages are installed and imported:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c879cd02-82db-441d-9476-fff1925bf494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprt opencv-python, NumPy, and matplotlib.pyplot\n",
    "try:\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "except:\n",
    "    ! pip install numpy, opencv-python, matplotlib # pip install if modules not present\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb45df38-e333-46b2-9161-80e6ac367532",
   "metadata": {},
   "source": [
    "### Image processing\n",
    "\n",
    "As in the previous tutorial, we will load in a 360x640 image and show the patch-normalized version.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f129b5-9a7a-4b50-9d94-b9bf512f8b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input image\n",
    "raw_img = cv2.imread('./mats/1_basicdemoquant/summer.png')\n",
    "rgb_img = cv2.cvtColor(raw_img, cv2.COLOR_BGR2RGB) # Convert to RGB\n",
    "\n",
    "# Load the patch normalized image\n",
    "patch_img = np.load('./mats/1_basicdemoquant/summer_patchnorm.npy', allow_pickle=True)\n",
    "patch_img = patch_img.astype(np.int32)\n",
    "# Create a figure to hold the subplots\n",
    "plt.figure(figsize=(10, 4))\n",
    "\n",
    "# Plot the first image\n",
    "plt.subplot(1, 2, 1)  # 1 row, 2 columns, 1st subplot\n",
    "plt.imshow(rgb_img)\n",
    "plt.title('Nordland Summer')\n",
    "\n",
    "# Plot the second image\n",
    "plt.subplot(1, 2, 2)  # 1 row, 2 columns, 2nd subplot\n",
    "plt.matshow(patch_img, fignum=False)\n",
    "plt.title('Nordland Summer Patch Normalized')\n",
    "plt.colorbar(shrink=0.75, label=\"Pixel intensity\")\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "max_int = np.max(patch_img)\n",
    "print(f\"The maximum integer pixel value is {max_int}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b68cf25e-35ae-4885-9cf1-c1b09ce4ad42",
   "metadata": {},
   "source": [
    "The patch normalized image here are floating point values in the range [0, 1]. For the base VPRTempo system, this is fine because the entire system works using floating points. However, in our quantized model we will be using integers. To demonstrate the conversion from floating point to integer, we'll manually quantize our input spikes by dividing using the `scale_factor` determined from the QAT.\n",
    "\n",
    "Let's load in some model scale factors, we'll use some of these later for the weight calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f67656a-3ba4-4374-b780-4e8bac4ec2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load network scale factor\n",
    "scale_factors = np.load('./mats/1_basicdemoquant/if_scales.npy',allow_pickle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82691809-7b06-4f0b-aca0-e19d293355b3",
   "metadata": {},
   "source": [
    "Like in the previous tutorial, we will convert this to a 1D-array to pass through the layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd6ae95-2a79-4b45-8a60-503079339739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 2D image to a 1D-array\n",
    "patch_1d = np.reshape(patch_img, (784,))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d9a5eaf-1de3-461f-b138-3ac820da8bae",
   "metadata": {},
   "source": [
    "### Load the pre-trained network weights\n",
    "\n",
    "Our network consists of the same architecture as in the previous tutorial. The excitatory and inhibitory weights have been converted to the integer representations from the QAT and will be applied directly to the quantized input spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d98749a-8f28-477b-871c-93626e96786c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input to feature excitatory and inhibitory network weights\n",
    "if_exc = np.load('./mats/1_basicdemoquant/if_exc.npy')\n",
    "if_inh = np.load('./mats/1_basicdemoquant/if_inh.npy')\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) # Adjust the figure size as needed\n",
    "\n",
    "# Plot the excitatory weights\n",
    "exc_plot = axes[0].matshow(if_exc.T)\n",
    "axes[0].set_title('Input > Feature Excitatory Weights')\n",
    "fig.colorbar(exc_plot, ax=axes[0], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Plot the inhibitory weights\n",
    "inh_plot = axes[1].matshow(if_inh.T, cmap='viridis_r')\n",
    "axes[1].set_title('Input > Feature Inhibitory Weights')\n",
    "fig.colorbar(inh_plot, ax=axes[1], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n",
    "# Print dtype\n",
    "print(f\"Excitatory weights integer type is {if_exc.dtype}\")\n",
    "print(f\"Inhibitory weights integer type is {if_inh.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de0fada-374c-4289-8042-5347b6b2ba21",
   "metadata": {},
   "source": [
    "In addition to this, we will set the zero points for these weights. From the QAT, the zero point for the excitatory weights was 0 and 127 for the inhibitory weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0405142-f8bd-4d83-a78e-d078f911d649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the zero point for the inhibitory weights\n",
    "zeropoint_inh = 127"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d591969a-e72e-43b2-8c89-16a13bb29fe6",
   "metadata": {},
   "source": [
    "### Propagate network spikes\n",
    "\n",
    "Now we'll propagate the input spikes across the feature to get the output, like in the previous tutorial. Let's start with the excitatory weights though first, since we will need to use different scaling of the output based on the zero point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c84239-c176-48c3-8954-25da5f989d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature spikes for the positive weight calculation\n",
    "exc_feature_spikes = (np.matmul(if_exc,patch_1d))\n",
    "\n",
    "# Now create the line plot\n",
    "plt.plot(np.arange(len(exc_feature_spikes)), exc_feature_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Excitatory Feature Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea0b0a3-66fc-4202-963c-cbd05114d283",
   "metadata": {},
   "source": [
    "We can see here that the spike values calculated for the feature layer are huge. That is because they need to be properly re-scaled after calculation. To do this, we need to take a couple of the scaling factors we imported earlier to transform the output to a reasonable range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd2b7f6-c698-4a7d-9099-da9cc59ed007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the required scale factors to transform the feature spikes\n",
    "perslice_scale_exc = scale_factors[0]\n",
    "perchannel_scale_exc = scale_factors[2]\n",
    "\n",
    "# Transform the feature layer spikes based on the scale factors\n",
    "scaled_exc_feature_spikes = (exc_feature_spikes//(perslice_scale_exc*perchannel_scale_exc))//perchannel_scale_exc\n",
    "scaled_exc_feature_spikes = scaled_exc_feature_spikes.astype(np.int32)\n",
    "# Plot out the scaled feature layer spikes\n",
    "# Now create the line plot\n",
    "plt.plot(np.arange(len(scaled_exc_feature_spikes)), scaled_exc_feature_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Excitatory Feature Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9d5a9e-05d3-475f-beec-1c6d29923cd5",
   "metadata": {},
   "source": [
    "Now let's do the same thing for our inhibitory weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1059733-2b2b-4c49-8ba2-2a52531f483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate feature spikes for the negative weight calculation\n",
    "inh_feature_spikes = (np.matmul(if_inh, patch_1d))\n",
    "\n",
    "# Get the required scale factors to transform the feature spikes\n",
    "perslice_scale_inh = scale_factors[1]\n",
    "perchannel_scale_inh = scale_factors[3]\n",
    "\n",
    "# Transform the feature layer spikes based on the scale factors\n",
    "scaled_inh_feature_spikes = ((inh_feature_spikes - zeropoint_inh) // (perslice_scale_inh * perchannel_scale_inh)) // perchannel_scale_inh + zeropoint_inh\n",
    "scaled_inh_feature_spikes = scaled_inh_feature_spikes.astype(np.int32)\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # 'figsize' can be adjusted as needed\n",
    "\n",
    "# First subplot\n",
    "axs[0].plot(np.arange(len(inh_feature_spikes)), inh_feature_spikes)\n",
    "axs[0].set_title('Inhibitory Feature Layer Spikes')\n",
    "axs[0].set_xlabel('Neuron ID')\n",
    "axs[0].set_ylabel('Spike Amplitude')\n",
    "\n",
    "# Second subplot\n",
    "axs[1].plot(np.arange(len(scaled_inh_feature_spikes)), scaled_inh_feature_spikes)\n",
    "axs[1].set_title('Scaled Inhibitory Feature Layer Spikes')\n",
    "axs[1].set_xlabel('Neuron ID')\n",
    "axs[1].set_ylabel('Spike Amplitude')\n",
    "\n",
    "# Adjust the layout\n",
    "plt.tight_layout()\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8e56a1-591c-4617-b3f0-2b41082b9a31",
   "metadata": {},
   "source": [
    "One thing you may notice is that although we used negative weights, we output positive spikes from in this operation. That is because of the `zeropoint_inh` of 127, which we add to the final spike calculation.\n",
    "\n",
    "Now that we separately calculated our positive and negative feature layer spikes, we need to add them together to get the final feature spikes. However, we'll note that because the scales and zero points for the two operations are different they will require to undergo additional transformation to match the scales. In the VPRTempoQuant model, we derive this addition scale and zero point from the `nn.quantized.FloatFunctional.add` function which learns these values during QAT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274df441-3015-4bab-a3bb-1498a2a7837a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combined scale factors\n",
    "combined_scale = 58\n",
    "combined_zeropoint = 61\n",
    "\n",
    "# Remove zeropoint from inhibitory spikes\n",
    "scaled_inh_feature_spikes_zero = scaled_inh_feature_spikes - zeropoint_inh\n",
    "\n",
    "# Combine the excitiatory and inhibitory feature spikes\n",
    "exc_rescaled = (scaled_exc_feature_spikes/perchannel_scale_exc) * combined_scale\n",
    "inh_rescaled = (scaled_inh_feature_spikes_zero/perchannel_scale_exc) * combined_scale\n",
    "print(perchannel_scale_inh.dtype)\n",
    "combined = (exc_rescaled.astype(np.int32) + inh_rescaled.astype(np.int32)) + combined_zeropoint\n",
    "combined = np.clip(combined,0,max_int)\n",
    "\n",
    "# Plot the combined spikes\n",
    "plt.plot(np.arange(len(combined)), combined)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Combined Feature Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e557a0ee-5202-45ac-af53-c49ccc65b4aa",
   "metadata": {},
   "source": [
    "Now we will apply the same process for the output layer to get the output spikes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4dc99-c7b9-4e9b-ba7c-58f6e30631cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the input to feature excitatory and inhibitory network weights\n",
    "fo_exc = np.load('./mats/1_basicdemoquant/fo_exc.npy')\n",
    "fo_inh = np.load('./mats/1_basicdemoquant/fo_inh.npy')\n",
    "\n",
    "# Create a figure and a set of subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10, 5)) # Adjust the figure size as needed\n",
    "\n",
    "# Plot the excitatory weights\n",
    "exc_plot = axes[0].matshow(fo_exc)\n",
    "axes[0].set_title('Feature > Output Excitatory Weights')\n",
    "fig.colorbar(exc_plot, ax=axes[0], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Plot the inhibitory weights\n",
    "inh_plot = axes[1].matshow(fo_inh, cmap='viridis_r')\n",
    "axes[1].set_title('Feature > Output Inhibitory Weights')\n",
    "fig.colorbar(inh_plot, ax=axes[1], shrink=0.4, label=\"Weight strength\")\n",
    "\n",
    "# Display the plots\n",
    "plt.show()\n",
    "\n",
    "# Print dtype\n",
    "print(f\"Excitatory weights integer type is {if_exc.dtype}\")\n",
    "print(f\"Inhibitory weights integer type is {if_inh.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b4f5f6-017b-4d7d-812a-c96baf9cb39f",
   "metadata": {},
   "source": [
    "We'll get our excitatory and inhibitory spikes for the output and scale them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780371ca-9dfe-4dd7-857d-e35be73ffd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the output layer scales\n",
    "fo_scales = np.load('./mats/1_basicdemoquant/fo_scales.npy',allow_pickle=True)\n",
    "\n",
    "# Calculate the excitatory and inhibitory spikes and scale them\n",
    "exc_output_spikes = np.round(np.matmul(fo_exc,combined))\n",
    "scaled_exc_output_spikes = exc_output_spikes // (fo_scales[0]) \n",
    "\n",
    "inh_output_spikes = (np.matmul(fo_inh,combined.astype(np.int32)))\n",
    "scaled_inh_output_spikes = (inh_output_spikes - zeropoint_inh) // fo_scales[1]\n",
    "\n",
    "# Combine the excitiatory and inhibitory feature spikes\n",
    "exc_rescaled = (scaled_exc_output_spikes/fo_scales[2]) * combined_scale\n",
    "inh_rescaled = ((scaled_inh_output_spikes - zeropoint_inh)/fo_scales[3]) * combined_scale\n",
    "\n",
    "output_spikes = (exc_rescaled.astype(np.int32) + inh_rescaled.astype(np.int32)) + combined_zeropoint\n",
    "output_spikes = np.clip(output_spikes,0,max_int)\n",
    "\n",
    "# Plot the combined spikes\n",
    "plt.plot(np.arange(len(output_spikes)), output_spikes)\n",
    "\n",
    "# Add title and labels if you wish\n",
    "plt.title('Combined Output Layer Spikes')\n",
    "plt.xlabel('Neuron ID')\n",
    "plt.ylabel('Spike Amplitude')\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd7e8f1-d9fc-48cc-82f6-6ca17c478c37",
   "metadata": {},
   "source": [
    "And now, as in the previous tutorial, we can clearly see that Neuron ID has the highest output spike amplitude corresponding to our first learned location.\n",
    "\n",
    "Let's quickly prove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2cf56e4-1bb8-47fd-869f-91e96c35f480",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output the argmax from the output spikes\n",
    "prediction = np.argmax(output_spikes)\n",
    "print(f\"Neuron ID with the highest output is {prediction}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8a7fb-66b4-455b-922e-b0fdc38b53c5",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "We have gone through a very basic demo of how VPRTempoQuant works and the operations involved for quantizing floating points spikes and weights into the integer domain. Although this is isn't exactly how PyTorch performs these tasks (a lot of them are done in the FP space, especially with regards to rescaling for addition) - it should give you a good idea as to how we can perform these kinds of operations in whole integers. This is particularly useful for implementation on hardware such as neuromorphic processors.\n",
    "\n",
    "If you would like to go more in-depth with training and inferencing, checkout some of the [other tutorials](https://github.com/AdamDHines/VPRTempo-quant/tree/main/tutorials) which show you how to train your own model and goes through the more sophisticated implementation of VPRTempo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b780c62e-53da-46c4-b882-f2dac2ca75ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
