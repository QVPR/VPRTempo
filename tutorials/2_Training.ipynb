{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a09c9e8-68f6-4edd-a8c1-1cc7f6b8bb00",
   "metadata": {},
   "source": [
    "## Training a new VPRTempo & VPRTempoQuant network\n",
    "\n",
    "### By Adam D Hines (https://research.qut.edu.au/qcr/people/adam-hines/)\n",
    "\n",
    "VPRTempo is based on the following paper, if you use or find this code helpful for your research please consider citing the source:\n",
    "    \n",
    "[Adam D Hines, Peter G Stratton, Michael Milford, & Tobias Fischer. \"VPRTempo: A Fast Temporally Encoded Spiking Neural Network for Visual Place Recognition. arXiv September 2023](https://arxiv.org/abs/2309.10225)\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this tutorial, we will go through how to train your own model for both the base and quantized version of VPRTempo. \n",
    "\n",
    "Before starting, make sure you have [installed the dependencies](https://github.com/AdamDHines/VPRTempo-quant#installation-and-setup) and/or activated the conda environment. You will also need the [Nordland](https://github.com/AdamDHines/VPRTempo-quant#nordland) dataset before proceeding, as this tutorial will cover training the network using this as an example.\n",
    "\n",
    "### Training new models for VPRTempo and VPRTempoQuant\n",
    "\n",
    "Let's start by training the base model with the default settings (if you have pre-trained a model, it will get removed for the purpose of the tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a757316e-4aa8-41aa-b03f-1ce4489d3705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the working directory to the main folder from tutorials\n",
    "import os\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff21590-9d00-4d8f-b860-ca1c0c849187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the base model with the default settings\n",
    "# If the pre-trained model already exists, we will remove it for the tutorial\n",
    "file_path = './models/VPRTempo313662725001.pth'\n",
    "\n",
    "if os.path.exists(file_path):\n",
    "    os.remove(file_path)\n",
    "    print(\"The file has been deleted.\")\n",
    "\n",
    "# Run the training paradigm\n",
    "!python main.py --train_new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285ca349-6295-4f04-bbe1-360d43111f82",
   "metadata": {},
   "source": [
    "Now we'll run the inferencing model to check and make sure our model trained ok."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1037e4ee-0361-4aa1-bfba-f979ffa51b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the base inferencing network\n",
    "!python main.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf8cd37-1af8-420f-ad76-481157a2920e",
   "metadata": {},
   "source": [
    "Great! Now let's have a look at changing a few of the default settings and training different kinds of networks. The default settings train 500 places, so if we want to only look at a smaller number of places we can parse the `--num_places` argument and specify how many places to learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81daf9c8-ea06-4e69-82a5-9179a7c38ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a new model with 250 places\n",
    "!python main.py --num_places 250 --train_new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a46cde0d-6343-48a4-87f8-aa5c9f56b231",
   "metadata": {},
   "source": [
    "And we can now inference using this smaller model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e2391-d0bc-4d58-bb35-8cf69ae08b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the inference for a model with 250 places\n",
    "!python main.py --num_places 250"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa835469-90cc-4f09-b7b4-688465321b18",
   "metadata": {},
   "source": [
    "Arguments for the base network work the same for VPRTempoQuant, we just need to also parse the `--quantize` argument. Let's now train another 250 place network, but also change a couple of other parameters. The default VPRTempo settings is a little slow to train on CPU, so let's reduce the image size from 56x56 to 28x28 and change the number of patches for patch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee89998-d334-4cdf-ac5b-88aed367ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a 250 place network with VPRTempoQuant\n",
    "!python main.py --quantize --num_places 250 --patches 7 --dims 28,28 --train_new_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5f3ce-acbb-435d-b197-f7e5841dcd70",
   "metadata": {},
   "source": [
    "And now we can inference this model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a9e948-5349-4714-b7d0-03ce694409b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run inference on newly trained VPRTempoQuant model\n",
    "!python main.py --quantize --num_places 250 --patches 7 --dims 28,28"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2648e38e-a0d7-481a-9009-3c4cf3e3fff0",
   "metadata": {},
   "source": [
    "### List of arguments you can parse\n",
    "\n",
    "The full list of arguments that can parsed to VPRTempo can be found in the `parse_network` function of `main.py`. Hyperparameters for VPRTempo are hardcoded into the layers and are not recommended to be changed since they generalize fairly well across multiple different datasets. \n",
    "\n",
    "### Conclusions\n",
    "\n",
    "This tutorial provided a simple overview of how you can train your own models for both VPRTempo and VPRTempoQuant, and changing a few of the network parameters.\n",
    "\n",
    "If you would like to go more in-depth, checkout some of the [other tutorials](https://github.com/AdamDHines/VPRTempo-quant/tree/main/tutorials) where we cover how to define your own custom dataset and work with expert modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bdc6d1-4690-4b48-a75f-f9d716cb7154",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
